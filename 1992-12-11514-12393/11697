Newsgroups: sci.crypt
Path: msuinfo!uchinews!linac!uwm.edu!spool.mu.edu!sol.ctr.columbia.edu!eff!news.oc.com!utacfd.uta.edu!rwsys!sneaky!gordon
From: gordon@sneaky.lonestar.org (Gordon Burditt)
Subject: Re: Cryptographic copy protection
Message-ID: <Byz911.2Lo@sneaky.lonestar.org>
Organization: Gordon Burditt
References: <ByxHyv.5s9.1@cs.cmu.edu>
Date: Wed, 9 Dec 1992 05:38:45 GMT
Lines: 92

>The point of entry I'm using is the fact that software is interactive.
>A single use of a piece of software doesn't reveal all of its intricacies 
>(unlike a musical number or a novel).  We can therefore hope to force
>a contact with a host machine H during each unique run of the program,
>enabling us to insist upon payment.

Contact with host machine H using the network as a remote dongle either 
(1) violates acceptable use polices of the network, or (2) limits your
possible customer base to way too small a market.  Depending on what
your software does, the Internet may be too small a market anyway.  Many 
businesses do not want critical functions, such as corporate accounting, 
connected to the Internet as the risk of things like worms or crackers 
is too great, thus your software won't run on those machines.

If you were a business executive, would you buy software that your
business depends upon for its functioning that would quit working
if (1) the network breaks, (2) the vendor's host breaks, or (3)
the vendor breaks (chapter 11)?   Your approach reminds me of the
license agreement in which purchaser agrees to fix all bugs in vendor's
software at purchaser's expense and sign over rights to the fixes
to the vendor, and the vendor doesn't agree that the software does
anything useful.

>	- a software provider with a host machine H
>	- an insecure but highly available network
>	- a user with a workstation U, not trusted by the software provider
>   Goal:
>	- a technique which can take a piece of software S and "securify" it
>	  into two parts, Su and Sh, so that developer can send Su to the
>	  user, run Sh on host H, and be assured of being paid for each
>	  use of the software.  I want a cryptographic guarantee, even
>	  against a determined disassembler and specialized hardware.  It
>	  is permissible to require a continuous active net connection
>	  between the user's machine U and the software provider's host H.

>3. A more general solution would be to include all of the relevant code in
>Su, but have it contact host H periodically, using public key cryptography to
>verify contact, and require H to agree to continued execution of the program.
>In this case, Su must be resistant to disassembly, or the user will just
>remove the part of the code that checks for approval by the host.  Since
>we seek cryptographic guarantees, this requires a "cryptographically strong
>code shuffler" which may or may not be possible under some reasonable set
>of assumptions.

This is going to be VERY difficult, as you have to assume that the OS
or in-circuit emulators can (1) single-step the program, (2) take snapshots 
of memory every instruction, (3) intercept, log, and possibly alter
every packet sent between the system and H, and (4) provide an architecture
where that data which is executable is not writable, and vice versa,
thereby prohibiting code from modifying itself in memory.  If your code
requires this ability, it can't run on this machine.

(1) is possible on a 386 or 486 in protected mode, and although a few 
instructions might reveal the single-stepping, a sufficiently smart debug 
handler could clean up the evidence before letting the program look at it.  
(2) can be implemented in the debug handler.  (3) can be implemented in 
the debug handler and/or the OS.  (4) is provided in the 386/486, and although 
it is possible to map executable and writable segments to the same memory 
location, the OS doesn't have to cooperate this way.  It can also trap and 
log all writes to code segments a bit more efficiently than the single-step 
method.  This doesn't disallow the program writing its own executables
and running them, but the executables can be saved while they're in 
existence.

>4. The ideal solution would be to have a self-contained piece of software S
>which need not contact host H at all; each time it is executed, some sort of
>"bit-rot" occurs, giving S a limited lifetime.  This is too ambitious; an
>opponent can defeat all such schemes by making extra copies of S before
>running it for the first time.

This description sounds suspiciously like many MS-DOS viruses.  I suspect
that it will be equally popular.

>The key to success is "cryptographically strong code shuffling" which
>gives a cryptographic guarantee against disassembly, or more precisely,

If the CPU can execute it, it can be disassembled.

>a guarantee that tampering will be detected.  Such a guarantee doesn't
>stop copying in the case where no contact with the host occurs, but
>does when the code requires a signed approval from the host before 
>continuing.  The code shuffling ensures that the code which contacts
>the host cannot be excised from the program without making it non-functional.

So who says the OS lets the application do self-modifying code?  Or
read its own code?  And the debug monitor can disable the checking code 
when the program enters the "contact host & verify" routine, and put it 
back after it gets past that point.  Getting to this point may require
some work, but it's not impossible.  

					Gordon L. Burditt
					sneaky.lonestar.org!gordon
