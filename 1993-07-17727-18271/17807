Newsgroups: sci.crypt
Path: msuinfo!agate!usenet.ins.cwru.edu!magnus.acs.ohio-state.edu!math.ohio-state.edu!wupost!csus.edu!netcom.com!netcomsv!cds8604!Cadence.COM!lou
From: lou@Cadence.COM (Louis K. Scheffer)
Subject: Re: Cryptanalysis of Compressed Text
Message-ID: <lou.742001525@cadence.com>
Sender: usenet@Cadence.COM (Usenet News)
Nntp-Posting-Host: caber.cadence.com
Organization: Cadence Design Systems, Inc.
References: <20sk9bINNkcv@twain.ucs.umass.edu> <16BFF13D3E.C445585@mizzou1.missouri.edu>
Distribution: na
Date: Tue, 6 Jul 1993 23:32:05 GMT
Lines: 24

C445585@mizzou1.missouri.edu (John Kelsey) writes:

>In article <20sk9bINNkcv@twain.ucs.umass.edu>
>quilty@twain.ucs.umass.edu (Lulu of the lotus-eaters) writes:
> 
>  I've seen the figure
>of 1 bit per character bounced around as the likely real information content
>of English, but I've never seen a good argument for why that's the case.

Check out Shannon's "Prediction and Entropy of Printed English", Bell System
Technical Journal, vol 30, Jan 1951, pp. 50-64.

The basic idea is simple - take long passages of text, and get smart people
to guess at the next letter, then the next, etc.  They are allowed to use
dictionaries, charts of 2 and 3 letter combinations, and so on.  The people
doing the guessing are explicitly trying to minimize the number of guesses
they make.  From the number of guesses required, you can calculate the
information content.  This appears to approach about 1 bit per character in
the case where at least the previous 100 characters in the string are known.

Hope this helps,

   -Lou Scheffer

