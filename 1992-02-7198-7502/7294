Path: msuinfo!uchinews!linac!pacific.mps.ohio-state.edu!zaphod.mps.ohio-state.edu!mips!news.cs.indiana.edu!lynx!nmsu.edu!opus!ted
From: ted@nmsu.edu (Ted Dunning)
Newsgroups: sci.crypt
Subject: Re: Measuring 'randomness'
Message-ID: <TED.92Feb9143807@lole.nmsu.edu>
Date: 9 Feb 92 21:38:07 GMT
References: <3298@wet.UUCP>
Sender: usenet@nmsu.edu
Reply-To: ted@nmsu.edu
Distribution: usa
Organization: Computing Research Lab
Lines: 94
In-Reply-To: naga@wet.UUCP's message of 9 Feb 92 03:40:40 GMT


sigh...

In article <3298@wet.UUCP> naga@wet.UUCP (Peter Davidson) writes:

   Some time ago I had reason to measure the "randomness" of binary
   data ...  so I devised a measure of randomness which I have found
   useful.

it would have been better to read knuth.

   If a file of N bytes really consists of bytes whose values are
   taken randomly from the byte space then we would expect to find
   that each byte value occurs approximately N/k times, or in other
   words, that the relative frequency of any particular byte value is
   1/k.

no.  only if the bytes are taken randomly with uniform distribution
and are independently chosen.  in general, all the the work you are
discussing is only valid for stationary independent sources.

   A measure of the randomness of the bytes in a non-empty file may be
   defined as follows:  the negative of the logarithm of the sum of the
   squares of the differences between the relative frequency of each
   byte value and 1/k, divided by the logarithm of the size of the
   file plus one.

using pearson's chi-squared test would have been better.  it allows
the test to be made versus any expected distribution.  it also has the
advantage that the distribution of the test statistic is well known
and thus estimates of significance can be made.

   the logarithms.  R is called the 'Dolphin randomness measure'.

this is humorous.  i wanted to make a caustic remark, but it is just
not worth it.

   Another approach to measuring randomness in a set of letters is due
   to Claude Shannon. ... where ln is the natural logarithm.

more commonly, base two is used so that the units are bits.

the formula you give is only the beginning and is only really
applicable when the letters are chosen independently.  if you had
actually read shannon's paper, you would have come to the section
where he talks about the non-independent case.

   For a DOS text file consisting of English text the relative entropy
   value (unlike the randomness value) is typically in the range 0.48
   - .68.  The relative entropy values for most non-random files,
   including .OBJ, .COM and .EXE files, usually fall in the range 0.50
   through 0.95.

these correspond to the well known fact that in english text, you have
about 4 bits of information per byte if you discount inter-character
dependencies.  if you account for these dependencies with a more
advanced model, you find only 1 bit of information per byte.

   Files consisting of bytes generated by pseudo-random-number
   generators typically have relative entropy values in the range
   0.970 - .999.

such simple tests cannot even detect an extremely bad
pseudo-random-number generator.  consider for instance the generator
which puts out 0, 1, 2, ... 254, 255, 0, 1 ...

obviously these numbers are not random, but if you calculate entropy
and use it as a measure, then you will find that the output appears
entirely random (i.e. 8 bits per byte).  the `Dolphin test' will also
show perfect randomness.

   A difference between the two measures of randomness is that
   relative entropy tends to be larger for larger files, whereas the
   Dolphin randomness measure seems to be independent of file size.

you are mistaken here.  your estimate of entropy only increases until
you have enough bytes to reasonably estimate the underlying
distribution.  until you have such a quantity, your estimate of
entropy will be lower than the actual value.  this effect shows up
quite clearly if you calculated the expected distribution of the
estimated entropy for given actualy probabilities.

   I shall be pleased to provide free copies of these to anyone who
   sends a blank 5.25" MS-DOS formatted disk to Dolphin Software.

how utterly kind.

if anybody is actually interested, i can post software which will do
you much more good for this sort of work.

also, i hate to bring this up again, but if the author of dolphin's
encryption software knows this little about the statistical analysis
of data, doesn't this say something devastating about the quality of
their software?
