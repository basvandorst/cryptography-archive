Path: msuinfo!uchinews!linac!uwm.edu!cs.utexas.edu!usc!elroy.jpl.nasa.gov!ames!apple!netcomsv!wet!wet.UUCP
From: naga@wet.UUCP (Peter Davidson)
Newsgroups: sci.crypt
Subject: Re: Measuring 'randomness'
Message-ID: <3298@wet.UUCP>
Date: 9 Feb 92 03:40:40 GMT
Sender: naga@wet.UUCP
Distribution: usa
Organization: Wetware Diversions, San Francisco
Lines: 96

> Subject: Measuring "Randomness"
> Date: 7 Feb 92 18:14:33 GMT
> How does one go about measuring the 'randomness' of binary data?
 
Some time ago I had reason to measure the "randomness" of binary data
(specifically, the files resulting from encryption by various methods)
and so I devised a measure of randomness which I have found useful.
 
Suppose that a file consists of N bytes, and the value of each byte is
contained in a set of k possible byte values (called the 'byte space').
An arbitrary file can contain bytes having any of any of the 256
possible byte values (so k may be 256) whereas a text file normally
contains only the usual upper and lower case letters, digits, punct-
uation and end-of-line markers (so for text files k may be in the range
70 - 100).
 
If a file of N bytes really consists of bytes whose values are taken
randomly from the byte space then we would expect to find that each byte
value occurs approximately N/k times, or in other words, that the
relative frequency of any particular byte value is 1/k.
 
A measure of the randomness of the bytes in a non-empty file may be
defined as follows:  the negative of the logarithm of the sum of the
squares of the differences between the relative frequency of each byte
value and 1/k, divided by the logarithm of the size of the file plus
one.  The relative frequency of the ith byte value (which may or may not
be the value i) is n(i) /N, where n(i) is the frequency of occurrence of
the ith byte value, and so we may express this mathematically as:
 
                    k-1
    R  =  ( - log( SIGMA ( p(i) - 1/k )^2 ) ) / log( N+1 )
                    i=0
 
where p(i) = n(i) /N.  The value R is independent of the base used for
the logarithms.  R is called the 'Dolphin randomness measure'.
 
R can be close to zero but can never be exactly zero. The minimum value
of R occurs when all bytes in the file have the same value, and in this
case R = log(k/(k-1)) / log(N+1).  If k = 256 then R = 0.0039 / ln(N+1),
where ln is the natural logarithm.
 
In the exceptional case that all byte values in the file occur exactly
the same number of times then p(i) = 1/k for all i and so the sum of the
squares is zero.  Since the logarithm of zero is not defined, there is
no R value defined in this case.
 
For a DOS text file consisting of English text the randomness value R is
typically in the range 0.20 - 0.40.  The randomness values for most
non-random files, including .OBJ, .COM and .EXE files, typically fall in
the range 0.20 through 0.65.
 
Files consisting of bytes generated by pseudo-random-number generators
typically have randomness values in the range 0.98 - 1.01.  Thus a file
with a randomness value of about 1.0 looks (at least according to this
measure) like a file consisting of random bytes.
 
Another approach to measuring randomness in a set of letters is due to
Claude Shannon.  The usual definition of entropy in a string of letters
from some alphabet was formulated by him in the 1950s.  Let S be a
string of letters from some alphabet A = { a(0), a(1), . . ., a(k-1) }
of k letters, and let p(i) be the probability (i.e. the relative
frequency) of occurrence of a(i) in the string S, then the entropy E of
the string S may be defined as:
 
                       k -1
              E  =  - SIGMA  ( p(i) * ln ( p(i) ) )
                      i = 0
 
where ln is the natural logarithm.  It can be shown that this value is
maximized when all letters occur in S with equal frequency (in this case
E = ln(k)), and is minimized when one letter occurs all the time (in
this case E = 0).
 
Since E ranges between 0 and ln(k), we may obtain a modified entropy
value E', called 'relative entropy', which ranges between 0 and 1
(similar to the randomness measure defined above) by dividing E by ln(k)
thus:  E' = E / ln(k).
 
For a DOS text file consisting of English text the relative entropy
value (unlike the randomness value) is typically in the range 0.48 -
.68.  The relative entropy values for most non-random files, including
.OBJ, .COM and .EXE files, usually fall in the range 0.50 through 0.95.
 
Files consisting of bytes generated by pseudo-random-number generators
typically have relative entropy values in the range 0.970 - .999.
 
A difference between the two measures of randomness is that relative
entropy tends to be larger for larger files, whereas the Dolphin
randomness measure seems to be independent of file size.
 
I have implemented both of these randomness measures in MS-DOS programs
that accept a file specification (wildcards permitted) and a byte space
value and return the randomness (or entropy) measures for the specified
file(s).  I shall be pleased to provide free copies of these to anyone
who sends a blank 5.25" MS-DOS formatted disk to Dolphin Software.
 
