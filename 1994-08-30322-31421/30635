Path: msuinfo!agate!howland.reston.ans.net!gatech!udel!hoz.telescan.com!RICK
From: RICK@univel.telescan.com (Rick F. Hoselton)
Newsgroups: sci.crypt
Subject: Re: Testing hardware RNG's
Date: Mon, 8 Aug 1994 10:53:35
Organization: Telescan
Lines: 27
Message-ID: <RICK.2.000AE4F2@univel.telescan.com>
References: <199408070637.BAA23590@pentagon.io.com>
NNTP-Posting-Host: hoz.telescan.com
X-Newsreader: Trumpet for Windows [Version 1.0 Rev A]

In article <199408070637.BAA23590@pentagon.io.com> ritter@io.com (Terry Ritter) writes:

>>However, for larger files, a small excess of "1" bits began
>>to show up (about 0.5% excess). 

> Averaging may appear to eliminate the bias, but I am not at all
> sure that it eliminates the problem.

> The bias is telling you something.  Don't hide it!  Find it
> and *then* decide what to do!

I agree that searching for the reasons for the bias is a very good idea.
But biased random numbers can still be useful.  The .5 % excess can be reduced 
to .005% by xor-ing two streams together, if the streams are truly independent 
and if the probabily of a zero or a one does not depend on previous bits.  
When these dependencies exist, if they can be found and modeled, then they can 
be eliminated (to any arbitrary precision) by arithmetic compression. Even a 
binary generator that is 90% predictable yields one bit of randomness for 
every 6.58 bits of output.

I do have a question for the real cryptananysts here, though.  How predictable 
would a one-time pad need to be before a large (plain English, for 
instance) plain-text could be recovered?  Or to rephrase the same question: 
How random does a one-time pad need to be to protect a large English 
plain-text?  (Measure randomness in predictability of each bit with 50% 
meaning "any guess is as good as any other", and 100% meaning "the bit is 
known for certain".)
