Path: msuinfo!agate!howland.reston.ans.net!darwin.sura.net!wvnvms!wvnvm!c445585
Newsgroups: sci.crypt
Subject: Re: making noise
Message-ID: <1701D6C6S86.C445585@mizzou1.missouri.edu>
From: C445585@mizzou1.missouri.edu
Date: Thu, 25 Aug 1994 01:39:15 -0400
References: <gradyCv0Fr4.22J@netcom.com>
Organization: University of Missouri, Columbia
Nntp-Posting-Host: 128.206.2.2
Lines: 29

In article <gradyCv0Fr4.22J@netcom.com>
grady@netcom.com (Grady Ward) writes:
 
>I suppose you could also expand on this principle to
>create a high quality random noise device from a large
>quantity of low quality devices that had the property of
>being able to be mass produced, several hundred or
>thousand on a die.
 
   Hmmm.  I suspect it would be more complicated than this, because
the low-quality randomness from each of the sources needs to be in-
dependent, doesn't it?  In principle, I cringe at trusting algorithmic
mechanisms to generate random numbers (which we only need because we
don't trust cryptographic random bit generators enough), but feeding
lots of independent low quality bit generators' output through a big
nonlinear combiner like many stream ciphers use might be a workable
idea.  This could probably be combined with an LFSR or something similar,
to make sure there was some source of balanced input bits in the worst
case.  Think of using one LFSR+weak random stream to shrink another,
or one to select which of the two others will be read for the next bit,
or to clock when the next read from the other stream will take place.
 
   Alternatively, collect the bits in a buffer somewhere and operate on
them with a block cipher in a CBC or CFB mode, or a hash function.  This
probably deals with most practical implementation problems, but it may
leave holes, in the sense that the algorithms we use may not really
be giving us "random enough" output.
 
>Grady Ward       |  For information and free samples on | "Look!"
 
   --John Kelsey, c445585@mizzou1.missouri.edu
