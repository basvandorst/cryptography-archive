Path: msuinfo!agate!ihnp4.ucsd.edu!swrinde!gatech!news-feed-1.peachnet.edu!news.duke.edu!eff!news.umbc.edu!olson
From: olson@umbc.edu (Bryan G. Olson; CMSC (G))
Newsgroups: sci.crypt
Subject: Re: **VERY FAST 1024BIT RSA**
Date: 31 Aug 1994 05:01:51 GMT
Organization: University of Maryland, Baltimore County
Lines: 75
Message-ID: <3412rv$h0v@news.umbc.edu>
References: <33s2f2$3p3@lucy.ee.und.ac.za> <33uhkc$fck@vanbc.wimsey.com> <phrCvD1z4.7qD@netcom.com>
NNTP-Posting-Host: umbc7.umbc.edu
X-Newsreader: TIN [version 1.2 PL2]

Paul Rubin (phr@netcom.com) wrote:
: In article <33uhkc$fck@vanbc.wimsey.com>,
: Mark C. Henderson <markh@vanbc.wimsey.com> wrote:
: >In article <33s2f2$3p3@lucy.ee.und.ac.za>,
: >Other tricks and techniques include:


: >2. Montgomery Multiplication. This is cool because you avoid doing
: >   mod/division, which is really nasty.

: An alternative to Montgomery multiplication for decryption is a
: speed-memory tradeoff: just precompute a table of multiples of powers
: of 256 (or 16, or 2) mod N.  Then to compute X^2 mod N, first compute
: X^2 using Karatsuba, then reduce it mod N by adding up the appropriate
: residues.  On machines with very slow multiplication, this looks like
: it might beat Montgomery at least for 512-bit modulus with CRT. The
: idea is that Montgomery replaces a multiplication and division with
: two multiplications, while the residue table replaces it with one
: multiplication and O(N^2) additions.

Maybe you can answer a couple questions.  

Readers who need the background see:
    Peter L. Montgomery, "Modular Multiplication Without Trial Division,"
    Mathematics of Computation, Vol. 44, April 1985 pp.519-521
or
    Antoon Bosselaers, Rene Govaerts and Joos Vandewalle, "Comparison of 
    Three Modular Reduction Functions," Crypto 93.

I like the basic Montgomery reduction algorithm, but his
multi-precision version seems worse than just doing multi-precision
multiplication in the basic reduction.  The multi-precision version
looks like it will always be Theta(n^2), while faster multiplies, like
Karatsuba, can be used in the basic version.  Does the multi-precision
version just have a better constant factor, or am I missing something
else ?

When you say it replaces a multiplication and division with two
multiplications, which version are you talking about ?  The basic
Montgomery reduction algorithm looks like it takes 2 multiplications
besides the one to get the product to be reduced.  Is there some way
to avoid one of these ?

: >3. Karatsuba (although you have to be very careful with this to
: >   get an advantage with 1024 bit numbers).

: It looks like quite a big win with numbers of this size on machines
: with slow multiplication (Intel x86).

There was a paper in an IBM Journal, for which I can't give a good
reference, which recommended Karatsuba and the tabular reduction, but
it was pretty long ago.

: >5. FFT (can anyone make this an advantage over simpler methods with 
: >   1024 bit numbers?) 

: It would surprise me a lot.  Any other views?

I agree it will be a while before we use numbers large enough to
benefit from the convolution methods.

: This might be more worth
: looking into for schemes like El-Gamal, where instead of computing x^s
: mod N (where s is constant but x is different in every instance of the
: problem), you are computing g^x mod P (where g is a fixed generator
: so you can do as much precomputation on it as you want).

There's a paper by Ernest F. Brickell, Daniel M. Gordon, Kevin S.
McCurley and David B. Wilson, "Fast Exponentiation with
Precomputation" which shows how to raise a constant base to different
exponents quickly.  I got a pre-pub version and I'm not sure where or
if the paper appeared.

--Bryan

