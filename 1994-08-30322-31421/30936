Path: msuinfo!agate!darkstar.UCSC.EDU!news.hal.COM!olivea!charnel.ecst.csuchico.edu!yeshua.marcam.com!MathWorks.Com!zombie.ncsc.mil!golf!mizzou1.missouri.edu!C445585
From: C445585@mizzou1.missouri.edu
Newsgroups: sci.crypt
Subject: Re: JMS: Don't do it!!!!!
Date: Thu, 18 Aug 94 19:03:08 CDT
Organization: University of Missouri, Columbia
Lines: 22
Message-ID: <1701610BF1S86.C445585@mizzou1.missouri.edu>
References: <eMIJkW37BT3M064yn@tdkt.tdkt.mn.org> <32th02$s5@spock.usc.edu> <32thv1$2n7@news.nd.edu> <32tvai$2bv@spock.usc.edu> <3302tf$q44@marlin.ssnet.com> <330qj2$92b@spock.usc.edu>
NNTP-Posting-Host: mizzou1.missouri.edu

In article <330qj2$92b@spock.usc.edu>
schillin@spock.usc.edu (John Schilling) writes:
 
>Good point.  My understanding is that gramatically correct English
>has an entropy of 1-2 bits per character.  Since we are dealing with
>a collection of proper names, with no relationship to one another, and
>with possibly alien spelling, it would probably be safe to assume 3 bits
>per character.  40-50 characters of such keys would probably suffice,
>100 definitely would.  Say, 10 names of 10 characters each.
 
   I think it's probably more complicated than this.  The entropy of english
text is 1-2 bits per character once it gets going.  I don't think the first
few characters are quite so low in entropy.  On the other hand, how many
proper names are there in a large dictionary of names?  If there are 2**16,
then 10 randomly selected names will give you 160 bits of entropy.  If the
names aren't chosen by a random process of some kind, then there will prob-
ably be fewer bits of entropy per name, but I don't have any idea how to get
an exact estimate....
 
>*John Schilling                    * "You can have Peace,              *
 
   --John
