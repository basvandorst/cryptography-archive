Path: msuinfo!news.mtu.edu!sol.ctr.columbia.edu!news.kei.com!MathWorks.Com!europa.eng.gtefsd.com!howland.reston.ans.net!EU.net!sunic!trane.uninett.no!nntp.uio.no!ma-mac17.uio.no!user
From: simen.gaure@math.uio.no (Simen Gaure)
Newsgroups: sci.crypt
Subject: Re: entropy
Date: Tue, 30 Aug 1994 12:56:32 +0100
Organization: University of Oslo
Lines: 47
Distribution: world
Message-ID: <simen.gaure-3008941256320001@ma-mac17.uio.no>
References: <1994Aug29.233527.1635@msus1.msus.edu>
NNTP-Posting-Host: ma-mac17.uio.no

In article <1994Aug29.233527.1635@msus1.msus.edu>,
mmorgan@solar.ee.stcloud.msus.edu wrote:

  Now, rand() does not produce random numbers, "especially
  on the lower bits", which seems to suggest that not
  only should one look at the numbers being generated, but
  one should also look at parts of the numbers being generated,
  right?
  
That's definitely true.  rand() isn't a very good random number
generator (even for non-cryptographic use).  The rand48() family
of generators are much better.  For cryptographic use, remember
that the 'randomness' of a pseudo random sequence is no better 
(and perhaps much worse) than that of the initial seed.
In many applications, the output of the random number generator
is subject to a modulus operation, i.e. you may happen to
only look at the lower bits.  (As an example, say you want
a dichotomous generator, then you typically only use the
lowest bit of the generator, it ought to be random too.)

Also, remember that 'entropy' is always measured relative to
a model.  I guess from your description that you're using
a Markov model, probably a 0th-order or 'Discrete Memoryless Source'-model.
As you point out, a necessary condition for a good uniform random generator 
is that the 0th-order entropy is maximal.  This is, as you point out,
not a sufficient condition.

E.g. take a sequence of 0's and 1's:
0 1 0 1 0 1 0 1 0 1...
Using a DMS-model you'll measure an entropy of 1 bit/symbol, but
using a 1st order Markov model (where you're allowed to look
at the previous symbol, not only the distribution of each symbol),
you'll measure an entropy of 0, i.e. you'll always know the next symbol.

The ultimate model for calculating entropy is 'reality', i.e. the
'perceived' entropy for the recipient (or crypt-analyst).  This
is hard to compute because there are many unknown factors, among
them the ingenuity of the analyst.

You may read about entropy and its relation to information content
in almost any introductory book on Information theory.

Btw, note that the unix program pack will compute the DMS entropy for
you.  (When you use the option '-' if I remember correctly).

-- 
Simen Gaure, Institute of Mathematics, University of Oslo
