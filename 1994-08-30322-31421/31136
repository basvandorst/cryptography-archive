Path: msuinfo!agate!darkstar.UCSC.EDU!news.hal.COM!olivea!charnel.ecst.csuchico.edu!yeshua.marcam.com!MathWorks.Com!europa.eng.gtefsd.com!ceylon!! ()
From: ()
Newsgroups: sci.crypt
Subject: Re: Authenticating pictures (long)
Date: 25 Aug 1994 02:16:45 GMT
Organization: GTE Laboratories Incorporated
Lines: 132
Distribution: world
Message-ID: <33guud$cdq@ceylon.gte.com>
NNTP-Posting-Host: 132.197.24.55

In message <170191376AS86.C445585@mizzou1.missouri.edu>, 
C445585@mizzou1.missouri.edu writes:

>In article <336p7s$nq8@mex0.jaist.ac.jp>
>brandt@chaos.frontnet (Jonathan Brandt) writes:
> 
>>Is there a way to authenticate a message as being "reasonably close" to
>>the original message?  I know this sounds like a contradiction, but the
>>application I am thinking of is in the transmission/reproduction of
>>images, where the transmission process alters the image slightly.
> 
>   A standard digital signature won't accept a change of even one bit.
>This is really a nice feature for most applications, but not necessarily
>for this one.  I think the best you could do would be a digital signature
>of some cannonical image format that all other formats could be reduced to
>(lower-resolution than any other format, probably) along with an algorithm
>for reducing any digital image to one and only one standard representation.
> 
This is a REALLY good question, and one that is all too often ignored when the 
subject of digital signatures comes up.

As some may know from some of my meanderings on pem-dev, one of the major 
problmes that I have been concerned about is what I call the Trusted WYSISG 
problem. That is, given that you know that the binary encoding of some object 
has not been altered since it was allegedly signed by the person whose 
certificate you think you have validated, how do you know that:

   a.  What the originating user signed was in fact a faithful representation   
of his intent? 

   b.  What you have printed out on your untrusted computer, using your 
unverified PostScript interpreter to decode what is in actuality a quite 
powerful computer programming language, is anywhere close to being a faithful 
representation of what the user allegedly signed?

This problem is difficult enough with just simple monospaced ASCII text. 
Unfortunately, the canonicalization proposed by PEM (and generally ignored 
elsewhere) doesn't require that hacks like space-backspace-space be deleted or 
flagged. Likewise, trailing blanks at the ends of lines, or blank lines at the 
end of the page do not require any special treatment. As a result, it is 
generally not feasible to take the printed output of a message together with a 
printed version of the signature, scan it back in on another computer, and then 
revalidate the signature. 

Now let's extend this to a desktop publishing document, printed using a 
standard PostScript interpreter. Now we introduce proportional spacing, perhaps 
variable leading between lines to make the columns line up properly, etc. Many 
casual PostScript users may not realize it, but some characters that might be 
an exact multiple of some number of pixels at high resolution, say 3000 dpi, 
may suffer from some truncation at lower resolution, say 300 dpi. As a result, 
and this is the bane of their existance for publishers who mock up a book using 
a laser printer, line breaks, and perhaps even page breaks, may come out 
differently, depending on the resolution of the printer being used! Imagine 
trying to convince a judge that two documents are identical, becasue the 
signatures are identical, when two different printouts don't even produce the 
same number of pages!

And how about font changes? Even discounting cheap ripoffs and clones of 
essentially the same fonts, there sure are a lot of them out there, so someone 
must think that they are important. If bold vs. normal vs. italic makes a 
considerable difference, how about semibold, or thin condensed? And if you 
think it doesn't really matter, try to imagine an IBM advertisement that is not 
printed in Bodoni Antiqua, their corporate typeface closen after probably 
megabucks of focus group analysis, but rather in say Hobo, or Park Avenue, or 
Lucida Blackface? Would it have a decidedly different semantic, if not 
syntactic, impact?

And how about a photograph that is printed using conventiona l(old-fashioned)  
half-tone techniques? A slight difference in the half-tone screen dot frequency 
can radically change the appearance of an apparently smooth, continuous image 
to one that approximates a silk screened poster.

Continuing down the line, now that we have digital scanners, we can easily 
correct the gamma or contrast ratio of the image to better fit the reproduction 
capabilities of the paper, ink, press, etc., etc., but we can change that 
contrast almost arbitrarily, at any point of the curve. Faint, subtle detail 
like the fold in a white shirt, or the proverbial black cat in a coal mine can 
be enhanced or deminished, whithout changing anything else. Everyone knows 
about the two pictures of OJ that appeared recently, one being somewhat darker 
and subjectively menacing, even though they were derived from a single unique 
piece of film. Were they the same?

So far we haven't even discussed noise in the reproduction, some of which might 
be considerably larger than the difference between a comma and a period in a 6 
point footnote. And how about the mechanical factors of skew, slewing, under or 
over magnification, etc., etc.?

After thinking about this for some time, I have concluded that the problem may 
be inherently that of a fractal, where the answer that you will get will depend 
substantially on the resolution that you are willing to insist on.

If so, then I THINK that the best that may be possible is to derive some sort 
of a pattern matching template, perhaps at some "reasonable but not extreme"
resolution, then to rescan the image and compare the result to the digitally 
signed template. At any reasonable resolution, the results can almost be 
guaranteed not to match, but perhaps a probabilistic judgement can be made.

Some of the the techniques that are being used to match (not classify) 
fingerprints might be applicable, but I don't know the technology. Fourier 
series, Modulation Transfer Functions -- I'm not anywhere close to being a good 
enough mathematician to comment.

However, having broken a number of cryptographic hashing algorithms published 
by very respectable authors, only to have my own solutions smashed in turn by 
Don Coppersmith, I would be VERY sceptical about the cryptographic efficacy of 
such solutions until a LOT of hard work had been put into analyzing them. 
Otherwise, it will probably be shown to be possible to rearrange, invert, etc. 
such a picture without any detection.

>  Now for the real ugly questions:  How do you design a string distance
>measure for an image that allows a little static, but not a meaning-changing 
>alteration of the image?

Since the "meaning" of the image will in general be determined by the 
recipient, at some time in the future, and under contexts that cannot be 
predicted in advance, I would submit that this is inherently undecidable. 
Somehow you are going to have to quantify the amount of tolerance that you will 
allow, and there are a LOT of variables to play with.

I sincerely wish you a lot of luck and creativity in solving this one, for I 
believe that the problem is both very important and very difficult.

Bob

Robert R. Jueneman
Mgr., Secure Systems
Wireless and Secure Systems Laboratory
GTE Laboratories
40 Sylvan Road
Waltham, MA 02254
1-617-466-2820 (rolls over to cellular if no answer -- have patience)

