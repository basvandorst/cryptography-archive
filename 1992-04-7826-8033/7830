Path: msuinfo!uchinews!linac!uwm.edu!wupost!waikato.ac.nz!aukuni.ac.nz!kcbbs!kc
Newsgroups: sci.crypt
Subject: Re: yet another Key Distribution IDEA !!
Message-ID: <36344.889219996@kcbbs.gen.nz>
From: Peter_Gutmann@kcbbs.gen.nz (Peter Gutmann)
Date: 29 Mar 92 10:05:44 GMT
Organization: Kappa Crucis Unix BBS, Auckland, New Zealand
Lines: 78

  
In article <1992Mar27.224629.8909@stephsf.com> a hypnotically implanted  
    suggestion made Bill England write:  
  
>  One of the encryption/compression methods I'm fond of is  
>  building compression models that can encrypt and compress  
>  your data at the same time.  While all of the models I have  
>  seen have flaws, I'm pretty sure that there will eventually be   
>  "good" models from an encryption standpoint.  
  
>  The standard method of "encrypting" using most models is  
>  to initialize the compression/state space based upon a key   
>  of some sort.  Large keys have the standand distribution   
>  problems, small keys are much eaiser to break, etc.  
  
I looked at this sort of encryption about a year ago, and as far as I can  
see it can't really be made to work.  Take the simplest example, and  
order-0 model with 256 symbols (for the nontechnical:  a Huffman  
compressor :-).  You skew the initial model by feeding your key through  
it.  Let's say your key is "Antidisestablishmentarianism" (nice long,  
complex word).  Now you use the model to compress some data.  This data  
happens to be binary, and is thus almost unaffected by the model, which  
has been skewed with an ASCII key (ie compressing with a model skewed by  
feeding it an irrelevant initial key is the same as compressing it with a  
normal model).  
  
This is even worse for dictionary-based compressors or higher-order models  
attached to statistical encoders - the strings or contexts present in the  
key may *never* occur in the data being compressed, which means the output  
from the compressor is identical to the output from a compressor with no  
fancy pre-conditioning for encryption.  
  
To achieve any sort of scrambling of the data, you would have to match the  
key *exactly* to the data being compressed, which is both impractical (it  
would severaly limit the choice of keys) and would require enormous keys  
to properly condition the model.  
  
Just out of interest, I used a simple order-0 arithmetic compressor for my  
tests.  This is a bit artificial since it's relatively easy to skew this  
sort of model, and in practice somthing this simple would never be used -  
it would almost always be a more complex model which would be very  
difficult to upset by priming it with key data.  But anyway:  
  
To initialise the model, get a key from the user.  Then compress the key  
using a std.model to get key-dependant whiteish noise.  Then feed this  
whiteish noise through the model to get a model which is dependant on the  
initial key (BTW you may recognise that I used this idea in NSEA too :-).  
This means that instead of getting only the symbols present in the key  
used for skewing, all symbols in the range 0..255 are used to skew the  
model.  The results for repeated encryptions of a sample (ASCII) test file  
using random keys were (averaged out over several dozen sample runs):  
  
  No.of white noise chars    No.of chars before there was a difference to  
  used to skew the model        data compressed with an unskewed model  
    
            1                             All chars identical  
            2                           Some hundreds of chars  
            5                                100-200 chars  
           10                                 8-16 chars  
           20                                  3-4 chars  
           50                                  1-2 chars  
          100                               All chars differ  
  
This shows that for even a hopelessly simple model like the order-0 one  
used here, a hundred or more chars of skewing are necessary to produce an  
observable change in the output data (you might need thousands if you were  
to look at individual output bits), and as mentioned above for a more  
complex model there is a fairly good chance you would *never* be able to  
skew it adequately to provide proper encryption without imposing severe  
restrictions in the keys.  
  
--  
    pgut1@cs.aukuni.ac.nz || peterg@kcbbs.gen.nz || peter@nacjack.gen.nz  
                           (In order of preference)  
Warning!  
  Something large, scaly, and with fangs a foot long lives between <yoursite> 
and <mysite>.  Every now and then it kills and eats messages.  If you don't  
receive a reply within a week, try resending...  
