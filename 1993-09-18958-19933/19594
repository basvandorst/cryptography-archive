Newsgroups: sci.crypt
Path: msuinfo!agate!dog.ee.lbl.gov!hellgate.utah.edu!fcom.cc.utah.edu!chemistry.utah.edu!bryner
From: bryner@chemistry.utah.edu (Roger Bryner)
Subject: Data compresion for scanned text documents
Message-ID: <bryner.14.748803490@chemistry.utah.edu>
Lines: 21
Sender: news@fcom.cc.utah.edu
Organization: University of Utah Chemistry Department
Distribution: sci.crypt
Date: Thu, 23 Sep 1993 16:58:10 GMT

Hey, I have a question I hope some of you people can help me with.  I am 
posting this to sci.crypt because I can think of no more appropriate place.
Please tell me if there is a better news group to post this question to.

I am scanning in a large number of documents using an automated setup.  
I will be converting these to text files using an OCR program,  but I also 
wish to keep the original binary files on tape backup.  These will be huge, 
about 5-10mb each.  They will also be highly compressible.  I am familiar 
with all of the standard compression algorithms, such as the ones used in 
ZIP and huffman coding.  These will probably not produce significant 
compression(less than 4-6 fold, I can think of algorithms which would do 
better than that. I would like 100 fold :^) because they do not take into 
account the two dimensional nature of the data.  	My question is 
this, 	What is the all time best compression method for scanned text, (no, 
the answer is not? jpeg).  The algorithms used by fax machines are probably 
too fast, and not efficient enough.
  	I am interested in source code, or a shareware or commercial 
program, with preference in that order
	I really hope I don't have to write this myself, it could be quite 
an undertaking.
	Thanks in advance for your help
