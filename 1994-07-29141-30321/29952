Path: msuinfo!netnews.upenn.edu!news.cc.swarthmore.edu!psuvax1!news.pop.psu.edu!news.cac.psu.edu!howland.reston.ans.net!math.ohio-state.edu!usc!bloom-beacon.mit.edu!senator-bedfellow.mit.edu!athena.mit.edu!solman
From: solman@athena.mit.edu (Jason W Solinsky)
Newsgroups: sci.crypt
Subject: Re: Variants of DES
Date: 22 Jul 1994 13:58:25 GMT
Organization: Massachusetts Institute of Technology
Lines: 56
Distribution: world
Message-ID: <30oja1$gue@senator-bedfellow.MIT.EDU>
References: <30grch$580@amhux3.amherst.edu> <30h1m8$bt0@senator-bedfellow.MIT.EDU> <30iqba$apb@zeus.london.micrognosis.com> <30iu2k$gqi@senator-bedfellow.MIT.EDU> <774812076Stu.stu@nemesis.wimsey.com>
NNTP-Posting-Host: e51-007-3.mit.edu

In article <774812076Stu.stu@nemesis.wimsey.com>, Stu@nemesis.wimsey.com (Stuart Smith) writes:
|> -----BEGIN PGP SIGNED MESSAGE-----
|> 
|> In article <30iu2k$gqi@senator-bedfellow.MIT.EDU> solman@athena.mit.edu (Jason W Solinsky) writes:
|> >I'm not sure I agree with this. I think that as long as you resign yourself
|> >to permuting known secure algorithms sequentially, its pretty unlikely that
|> >you will wind up with an algorithm that is less secure than the most secure
|> >of the well known algorithms that you permute. In general, to do this with an
|> >existing code library requires a VERY small amount of additional coding
|> >in which bugs can appear.
|> 
|> But how do you know that the algorithms your are using one after
|> another are not interacting to create a hole?  Unlikely as hell,
|> but if your sequence of algorithms isn't subjected to public
|> scrutiny, if such a hole exists the only people to find it will
|> be your opponents.

Well here a stronger claim: If you use a two known secure ciphers back to
back, and these two ciphers are unrelated to each other, there is a ZERO
percent chance that they will decrease your security. And they add quite a
bit in the way of obscurity.

|> >I would claim that the most important reason why a combination of known secure
|> >attacks is a good idea is that it provides a little obscurity without forcing
|> >you to use an untested algorithm. If the NSA is all set up in its basement with
|> >its anti-DES machine, and you throw in a little key-based twist before or after
|> >the encryption, their hardware is going to require serious hacking to make it
|> >work on your new key.
|> 
|> Obscurity is usually what we try to avoid.  If the NSA can crack
|> any given DES data stream, then "a little obscurity" is most
|> certainly *not* going to stand it in its way.  "serious hacking"
|> simply does not compare in difficulty to anything requiring 2^56
|> operations.

I disagree for two reasons:

A) If the NSA would crack your primary encryption algorithm with a brute force
device, its going to be a very special purpose device. If you alter your
algorithm enough, its just not going to work.

B) The ability to break through encryption does NOT imply the ability to break
through obscurity. You need some access to both participants machines. In a
highly networked society, the two participants could agree on a type of
obscurity to layer on top of the security, use it and erase it, without any
human intervention. The data can then be evesdroped on, and the machines used
to send it commandered, and the NSA is screwed. Attacking secure algorithms,
even DES, takes time and (in most cases) specialized hardware. If you don't know
precisely how things were encrypted, you just can't do this.

Even a simple technique based on the hash of a shared password could easily
fry an atempted brute force attack. Its so easy to add a little obscurity to
your algorithm, I don't think there is any good reason not to (besides
standardization type issues.)

JWS
