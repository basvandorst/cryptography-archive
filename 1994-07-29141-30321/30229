Path: msuinfo!netnews.upenn.edu!dsinc!spool.mu.edu!howland.reston.ans.net!gatech!news-feed-1.peachnet.edu!news.duke.edu!MathWorks.Com!news2.near.net!bloom-beacon.mit.edu!senator-bedfellow.mit.edu!athena.mit.edu!solman
From: solman@athena.mit.edu (Jason W Solinsky)
Newsgroups: sci.crypt
Subject: Re: weak encryption+compression+strong encryption=???
Date: 28 Jul 1994 22:34:55 GMT
Organization: Massachusetts Institute of Technology
Lines: 78
Distribution: world
Message-ID: <319bqf$7j8@senator-bedfellow.MIT.EDU>
References: <318m0n$6ij@news.uncc.edu>
NNTP-Posting-Host: e51-007-3.mit.edu

In article <318m0n$6ij@news.uncc.edu>, goodrum@unccsun.uncc.edu (Cloyd Goodrum) writes:
|> 	The standard argument against compressing data after
|> encryption is that a decent encryption algorithm will not
|> leave enough redundancy in the text to make the data 
|> compressable.

Lets be a little stronger than that. If your compression algorithm works
at all after encryption, stay clear of that encryption algorithm.

|> 	But what if you encrypted again, this time with a 
|> stronger algorithm, after encrypting with a simple substitution
|> cipher (which would leave the data with the same statistical
|> properties it had before) and then compressing it? Would that
|> buy you anything? Would you be better off, worse off, or the
|> same as if you had merely compressed and then encrypted?

You'd be as secure as you were before.

|> 	Also, some compression algorithms can compress data
|> compressed by other algorithms. For instance, it's my understanding
|> that data compressed with Lempel-Ziv can be later compressed with
|> huffman code or arithmetic coding. What if you had something 
|> like the following:
|> 
|> substitution cipher+Lempel-Ziv+substitution cipher+Huffman Code
|> +strong encryption (such as IDEA)

First of all, you need to understand why compression before encryption is so
important. Compression decreases the number of bits in the plaintext without
decreasing its entropy. When somebody tries to cryptanalyze your ciphertext,
if they don't have access to the corrosponding plaintext they have to look
at the result to figure out whether or not they found the correct key. The
assumption that the cryptanalyst has to make is that the redundancy in the
plaintext will cause any incorrect key to result in blather. But if your
compression has removed most of the redundancy, even incorrect messages may
look like english text. The computer will have no way to figure out which
is the correct message.

For example, a program I am working on is designed to work with average PCs
receiving data over a phone line (call it 20 kbps). Both the sender and the
receiver have a 64K hash table which is in essence a static Markov chain that
can be scanned through extremely quickly. This is used to predict the
probability distribution of the next character, and implement a quick variation
on arithmetic coding. This table is modified by a small dynamic adjustment
that is transmited with the file. It achieves 5-6 times compression on
randomly selected HTML files. (At a speed penalty that would prevent this
from being useful over high speed internet connections.)

Now if you send a random bitstream into this thing, what you get out is going
to look roughly like HTML. Humans will have no trouble figuring out that its
gibberish, but computers will have a hell of a time. And if you are going
through any significant number of keys, you will wind up with several files
that take humans a significant amount of time to analyze. If you are dealing
with short files, you are going to wind up with multiple gramatically correct
results. This makes cryptanalysis enourmously difficult if not impossible.

Now, returning to the techniques you suggested, LZ compression does not have
this property. Both 77 and 78 based compression produce total gibberish when
used on random bitstreams. They weren't intended to come close to maximizing
entropy/bit. They were intended as quick hacks that could be implemented in
software or used in high speed hardware and still get better than 2:1
compression on many types of files. In general if you want a form of
compression that makes the encryption stronger, you want to use a quick
version of arithmetic coding (probably one based on shifting) along with a
small dynamic or static probability predictor. These have the potential to
do REALLY nasty things to cryptanalysis.

One character at a time Huffman coding will reduce most LZ files in size (as
long as they pick their code tree either dynamically or statically but based
on the file at hand), but reguardless of the speed/security tradeoff you are
looking for there is almost certainly a better way. In general if you are
using two compression routines, its because nobody had the time to write
one that did what you wanted.

As I'm sure others will tell you, substitution ciphers and anything else that
allows simple compression, hardly afford any protection at all.

JWS
