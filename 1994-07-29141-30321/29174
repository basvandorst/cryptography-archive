Path: msuinfo!netnews.upenn.edu!news.amherst.edu!news.mtholyoke.edu!news.byu.edu!gatech!swrinde!elroy.jpl.nasa.gov!lll-winken.llnl.gov!decwrl!parc!biosci!headwall.Stanford.EDU!eillihca@drizzle.Stanford.EDU
From: eillihca@drizzle.StanFord.EDU ( Achille Hui, the Day Dreamer )
Newsgroups: sci.crypt
Subject: Re: **** Decrypting challange stage 2 ******
Date: 30 Jun 94 20:12:07 GMT
Organization: Dept. of Physics, Stanford University.
Lines: 56
Message-ID: <eillihca.94063013120728625@drizzle.Stanford.EDU>
NNTP-Posting-Host: drizzle.stanford.edu
X-Transfer-Agent: nntp.stanford.edu (NNTP)

+---sehari@iastate.edu (Babak Sehari)-writes---
|
| -----
| 
| Greedings,
| 
| ... and the winner of the challenge is:
	     ^^^^^^^
      Not really, I'm just cheating ;-)
| 
|   eillihca@drizzle.StanFord.EDU ( Achille Hui, the Day Dreamer )
| 
| Really, good. How did you do it?
| 
As I said, I haven't crack the code. When I compare the set of plain texts
and secret texts, what strikes me is that not only the secret texts are much
bigger than the corresponding plain texts, their sizes correlates extremely
poorly with the sizes nor the entropies of the plain texts. This suggest the
plain texts have been somehow preprocessed before feeding to the actual
encoder. So I apply various common UNIX filters to the plain texts and study
the corr. sizes. It turns out the sizes of the gzipped plain texts correlates
extremely well with those of the secret texts:

    size(gzipped) = round(0.493064 * size(secret)  -  84.278519) +/- 2
    size(secret)  = round(2.028078 * size(gzipped) + 170.973586) +/- 5
				   
The linear correlation coefficient is 0.999986 which is 3 order of magnitude
better than everything else.  Using this, I estimate the size of the gziped
plain text should be 4001 +/- 6 bytes. I go and grab all the articles in the
sci.crypt, select those whose gzipped sizes close to 4000 bytes, hand-edit the
path to make it closer to the real one and gzipped them again. There is one
and only one article whose gziped size (3998 bytes) falls within this range
and the next one missed at least 300 bytes.

Here are some other observations: 

       [1] 2.028134 which is very close to 2
       [2] the sizes of all the secret texts is even
       [3] the exceptional high incident count at 2,4,... observed by 
	   another netter (sorry, I forget who you are).
	
I think the encoding procedure consists of gziping the plain text, create an
array of short integers, one from each byte of the gzipped files and processes
the data in unit of short integers. What sort of transformation needs to
represent the alphabets as short integers? I bet somewhere modular arithmetics
is involved, may be in the form of Hill's transform or certain sort of 
knapsack problem. Of course, if I know the encoding algorithm, I don't need
to deal with all these messes in figuring out the file format ;-)
----------------------------------------achille (eillihca@drizzle.stanford.edu)


	
	



