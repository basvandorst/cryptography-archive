 F4 Hrel.  Compute Relative Entropy (A Brief Tutorial)

 This function computes and displays the relative entropy associated with the
 content of a disk file.  Note that this concept applies to file content and
 not to the file per se (a file is a DOS "thing").  However, it is convenient
 to speak of the entropy or the relative entropy of a file and that convention
 is used here.

 Relative entropy computation is an example of a one way hash function.  In
 terms of representation, the content of a disk file is just a byte string.
 Passing a file through the relative entropy function produces a number that
 is characteristic of only that file.  The computation is "one way" in the
 sense that the byte string (i.e., file content) cannot be recovered from
 the number:  byte string --> Hr is easy, Hr --> byte string is impossible.

 For obscure historical reasons the mathematical symbol for entropy is H.  The
 symbol E seems more appropriate to English speaking people.  Obscuring matters
 further, the utility computes and displays "relative" entropy.  These are the
 reasons behind the somewhat cryptic symbol Hrel (also referred to as Hr).

 The concept of entropy emerged from the physical sciences.  In that context
 entropy is a measure of disorganization and represents one statement of the
 second law of thermodynamics.  In information theory entropy is often used
 as a measure of uncertainty.  In either case, the greater the disorganization
 or the greater the uncertainty, the higher the entropy.  A simple coin tossing
 example will make this idea qualitatively clear.

           H     T        H     T        H     T        H     T
          ---   ---      ---   ---      ---   ---      ---   ---
          0.5   0.5      0.7   0.3      0.9   0.1      1.0   0.0

             (a)            (b)            (c)            (d)
 
 Figures (a) through (d) given above are referred to as finite schemes.  Each
 finite scheme is comprised of two events, heads (H) and tails (T), and the
 probabilities assigned to the events.  Expressing probabilities in colloquial
 terms we have (a) 50-50, (b) 70-30, (c) 90-10, and (d) 100-0.

 The first coin, (a), is obviously fair.  When one flips this coin there is no
 a priori reason to call heads (H) over tails (T).  The coin is as likely to 
 come down on one side as the other and, in the long run, one would expect the
 ratio of heads to tails to approach 1.0.

 The biased coin (b) has a marked preference for heads.  In the long run one
 would expect to see more heads than tails.  There is a very good reason to
 call heads when this coin is flipped: it's "probably" the correct call most
 of the time.

 The probabilities assigned to the third coin, (c), reveal an extreme bias for
 heads.  Heads dominate even in the short term.

 The fourth coin, (d), is obviously one with two heads and no tail.  In the
 absence of other criteria distinguishing one side from the other, flipping
 this coin represents no element of chance whatsoever.  It will always land
 heads up.

 Considering each coin in turn, it is seen that varying degrees of certainty
 are present.  The fair coin (a) represents the case of complete uncertainty;
 coin (d) is associated with absolute certainty.  Coins (b) and (c) represent
 degrees of certainty, or uncertainty, intermediate between the two extremes.

 Observe that the probabilities assigned to each coin lie between 0.0 and 1.0
 inclusively and, in each case, sum to 1.0 (e.g., 0.7 + 0.3 = 1.0).

 The observed properties are those of a discrete probability space.  The space
 associated with coin (a) is uniform; all (in this case both) probabilities are
 equal.  The spaces associated with (b), (c), and (d) increasingly deviate from
 uniformity.

 That's the qualitative part.  The quantitative part can be described in three
 steps:

    - form the product of each probability with its logarithm.
    - sum all products to produce the entropy of the scheme.
    - Divide the maximum entropy Hmax into the entropy of the
      finite scheme (Ha, Hb, Hc, or Hd) to produce relative
      entropy Hr.

 Here are the results for finite schemes (a) through (d).  Since there are two
 events per finite scheme, heads or tails, Hmax = log2:

      Scheme (a).  Ha = -(0.5log0.5 + 0.5log0.5) = -2(0.5log0.5) = log2
                   Hr = Ha/Hmax = log2/log2 = 1.0 (TOTAL UNCERTAINTY).

      Scheme (b).  Hb = -(0.7log0.7 + 0.3log0.3) = 0.265295
                   Hr = Hb/Hmax = 0.265295/0.301024 = 0.881291

      Scheme (c).  Hc = -(0.9log0.9 + 0.1log0.1) = 0.141182
                   Hr = Hc/Hmax = 0.141182/0.301024 = 0.468996

      Scheme (d).  Hd = -(1.0log1.0 + 0.0) = 1.0(0.0) + 0.0 = log1.0
                   Hr = Hd/Hmax = 0.0/log2 = 0.0 (ABSOLUTE CERTAINTY).

 Utilization of this concept requires a computational procedure to derive
 representations of finite schemes from the contents of disk files.  The
 procedure, called normalization, is straightforward.

    (1) Read the file into memory and count the occurrence of each ASCII
        character.  The utility uses the complete ASCII character set so
        there are 256 character codes numbered 0 to 255 (or 00 to FF hex).
        In the coin tossing experiment two "events" are possible, heads
        or tails, and the maximum entropy associated with this experiment
        is log2.  When counting characters in a file, 256 "events" are
        possible.  That is, each ASCII character corresponds to an event.
        Hence, the maximum entropy of the ASCII character set is log256. 

    (2) Sum the character counts in the variable Sum.

    (3) Take the reciprocal of the sum: k = 1 / Sum.

    (4) Multiply each character count by k.

 This four step procedure scales each character count to a value between 0.0
 and 1.0 and all of the values sum to 1.0.  Normalization produces the finite
 scheme.  Having derived the finite scheme, it is a simple matter to compute
 the entropy of the file by summing the products of the normalized character
 counts with their respective logarithms.

 The entropy of the ASCII character set, log256, is then divided into the
 entropy computed from the finite scheme to produce the relative entropy, a
 number between 0.0 (maximum certainty) and 1.0 (maximum uncertainty).  

 Here are the implications.

   - Relative entropy Hr = 1.0 is associated with a uniform probability
     space and indicates that ASCII characters are distributed throughout
     the file in equal numbers.

   - Encrypted files exhibit Hr > 0.99 (e.g., 0.995537864).  The fact that this
     number is very close to 1.0 implies that ALL of the ASCII characters are
     present in the file in roughly equal numbers.  This, in turn, means that
     correlations relating the frequency of occurrence (and relative positions)
     of characters between plaintext and ciphertext do not exist.  As a case in
     point, plaintext file text.doc (supplied with the utilities) contains one
     end-of-file character and 552 spaces.  After encryption the file contains
     approximately 20 end-of-file characters and, more or less, an equal number
     of spaces.

 In other words, ciphertext with a computed relative entropy greater than 0.99
 is not amenable to cryptanalytic attacks based upon statistical methods.  This
 is an important result because statistical methods are among the most powerful
 cryptanalytic tools.

 Two further points concerning relative entropy are important:

    - Relative entropy computed from small files (less than 1K bytes) is
      usually less than 0.99.  The reduced value is due to the fact that
      small files lack the quantity of data required to drive the value up
      to 0.99 and beyond.  Nevertheless, encryption is as effective for
      small files as it is for large ones.

    - for files greater than 10 or 12 bytes in length, the relative entropy
      of plaintext is always less than that of the corresponding ciphertext.

 The following table gives typical values of relative entropy by file type.

                       File type              Hr  
                       -----------------  -----------
                       plaintext text     0.60 - 0.66
                       plaintext binary   0.80 - 0.86
                       all ciphertext     0.99...

 Hr is displayed to nine places after the decimal point.  The computation
 detects a polarity change of one bit in eight million (or better!).  This
 degree of sensitivity can be put to good use.

 The OTP development system is periodically scanned for viruses.  One likely
 candidate for infection is the DOS program COMMAND.COM.  As an added safety
 measure, a copy of E.EXE is stored in the root directory and the batch file
 command "E COMMAND.COM" is included in AUTOEXEC.BAT.  Each time the system is
 booted, E.EXE reports the relative entropy, program name, and file size as

                   Hr=0.839647705 COMMAND.COM 47845 bytes

 An unaccountable change in this number is sufficient cause to suspect the
 presence of an intruder.

 The relative entropy command line utility, E.EXE, was developed as a Codeword
 V2.0 analysis tool.  At the time, it was required to produce quantitative
 results concerning the distribution of ASCII character codes in ciphertext.
 E.EXE is included in OTP V1.1 in order to provide a simple and reliable method
 of determining the textual state (plaintext or ciphertext) of any file stored
 in your system.

 Coding theory, an information theoretic approach to minimizing redundancy in
 the representation of information, provides the theoretical foundation for the
 various data compression schemes available today.  In this context, entropy
 serves to impose a theoretical limit on the extent to which a given file can
 be compressed.

 Lossless compression models (those capable of producing perfect expansions of
 compressed data) have no positive effect on data that can be represented by a
 uniform finite scheme.  On the other hand, the greater a scheme deviates from
 uniformity, the greater the potential for compression.  Hence, if you decide
 to compress ciphertext to reduce the time and cost associated with sending
 data through a modem, don't!  Instead, compress the plaintext first and THEN
 encrypt the file.

 The relative entropy associated with OTP produced ciphertext is consistently
 greater than 0.99.  This means that the underlying discrete probability space
 is virtually uniform which, in turn, means that compression programs will have
 little or no effect on ciphertext.  As an exercise, why not encrypt text.doc
 and then attempt to compress the ciphertext.  The result may surprise you.
 Using PKZIP 2.04g to compress text.doc ciphertext actually increased the file
 size from 4208 bytes to 4322 bytes.  This is not an indictment of PKZIP 2.04g.
 Compression works because programs like PKZIP recode the information content
 of disk files in an effort to minimize redundancy.  Encryption tends to hide
 redundancy without reducing file size.

 Typically, a compression program builds a table of probabilities from the
 input of the file being compressed.  The most frequently occurring characters
 are assigned short codes (2 to 4 bits in length) while characters that occur
 only several times are assigned long codes (perhaps 14 to 18 bits in length).
 If a file is compressible, short codes dominate resulting in a reduction in
 file size.  In an encrypted file, all of the ASCII characters are present in
 more or less equal numbers (i.e., a uniform finite scheme).  Therefore, it is
 not possible for a compression program to assign either short or long codes
 as replacements for the ASCII characters.  The redundancy in the ASCII coding
 scheme is still there, but it cannot be removed by the compression program.
 The file is incompressible.  Of course, the compression program must include
 the table of probabilities in the "compressed" file for the benefit of the
 expansion program.  Adding the table of probabilities to a file that will not
 compress serves only to increase the file size.  This is why attempting to
 compress an encrypted copy of text.doc results in actually increasing the size
 of the file from 4208 to 4322 bytes.  The additional 114 bytes represent the
 expansion table included by the compression program.

 Finally, it is worth noting that keys produced by the OTP nonlinear random
 number generator are not compressible.  This is a mandatory characteristic
 of any random number generator used as the basis for a one time pad cipher
 implementation.

 -----------------------------------------------------------------------------
