Newsgroups: sci.crypt
Path: msuinfo!caen!sdd.hp.com!cs.utexas.edu!milano!cactus.org!ritter
From: ritter@cactus.org (Terry Ritter)
Subject: Re: PGP *2.0* available
Message-ID: <1992Oct6.091329.20779@cactus.org>
Organization: Capital Area Central Texas UNIX Society, Austin, Tx
References: <920929152106_74076.1041_DHJ67-1@CompuServe.COM> <BvnqH7.32C@exnet.co.uk>
Date: Tue, 6 Oct 1992 09:13:29 GMT
Lines: 73


 In <BvnqH7.32C@exnet.co.uk> s0rah@exnet.co.uk (R A Hollands) writes:

>> But I guess you mean that the cryptosystem can be less than secure
>> if we only use it for small secrets.  I never know how to respond
>> to this:  We have to set up a secure link *before* we use it.  Can
>> we know the extent of the secrecy we will need in the future?  Can
>> we afford to squander secrecy up front simply because we are lazy
>> and would prefer not to validate public keys?  I wouldn't think so,
>> but I'm certainly willing to listen.

>(Just yell if "I really want to get out of this discussion" still applies)

 It's not permanent, it's just that composing a serious response
 for each of many different postings seriously limits real life.
 I'm still catching up on the latter.


>If we expand "small secrets" so that our secrets are evaluated in terms of
>the cost to us of their being compromised - big secrets, high cost - then
>doesn't it make sense to spend only as much on secrecy as we think our
>secret is worth?

 OK, but exactly what is the operational "cost" of a spoofing
 process running on somebody else's computer?  What kind of
 secret costs less than sending "new key" messages to both ends?
 (I see no reason to imagine that this would not succeed if the
 ends never validated their keys originally.)

 Certainly, no such program can handle *all* situations, but those
 messages it could not handle could be detoured for human action.
 Then, once spoofing has been accepted by both ends, deciphering
 and re-enciphering should be automatic and invisible to the users.
 This is cheap, clean, white-collar intelligence; a real threat.


>Since "absolute" security is absolutely unattainable aren't we actually
>obliged to do this (unless we're in it for amusement sake only)?

 True, absolute security is a goal; not reality.  But it would
 seem rather strange to argue that this means that we don't need
 to worry about enforcing security (since we know it can never be
 "absolute").  It seems to me that we need to fix every hole we
 can find, but most especially the cheap and easy holes.


>Isn't "good enough" good enough?  PGP seems good enough to me, but, then,
>I've only got small secrets.  And a public key posted here is just a
>supply of envelopes.

 Again, PGP is *not* the problem.  The problem is *users* who do
 not validate their public keys.

 A public key posted here may or may not be spoofed.  If the key is
 not spoofed, then we save all the trouble of validating the key.
 But if the key *is* spoofed, then there is no secret at all.  There
 is *no* envelope; the subsequent conversation is *completely* open.

 Thus, the "good enough" aspect of non-validated keys mainly acts
 to enable the construction of a spoofing system.  Once the system
 is constructed, it can be applied virtually for free.  This does
 not mean that *everyone* could be attacked, but since we could not
 know who *would* be at risk, we all have to accept the possibility.
 When spoofing is applied, the "cost" of attacking a non-validated
 key is almost zero.

 Why would anyone bother using a cryptosystem if they will only
 use it in a way that can be completely negated almost at will?

 ---
 Terry Ritter   ritter@cactus.org


