Xref: msuinfo sci.crypt:9921 alt.security:8181
Path: msuinfo!uchinews!linac!uwm.edu!zaphod.mps.ohio-state.edu!sol.ctr.columbia.edu!destroyer!gatech!wrdis01!ocdis01!robjohn
From: robjohn@ocdis01.UUCP (Contractor Bob Johnson)
Newsgroups: sci.crypt,alt.security
Subject: Re: Letter Frequency
Message-ID: <68@ocdis01.UUCP>
Date: 16 Oct 92 14:16:58 GMT
References: <1big1qINNrnq@matt.ksu.ksu.edu> <1992Oct15.140918.27296@emr1.emr.ca> <1992Oct15.145339.2020@fid.morgan.com>
Followup-To: sci.crypt
Organization: Tinker Air Force Base, Oklahoma
Lines: 54

In article <1992Oct15.145339.2020@fid.morgan.com> sethb@fid.morgan.com (Seth Breidbart) writes:
>>  grep -i e /usr/dict/words | wc
>Exercise for the reader: Identify two errors in the methodology.

1. A dictionary is not representative of actual usage.
2. Method gives a count of words which contain each letter, not letter freq.
3. The list isn't sorted.
4. Most dictionaries only contain root words - prefixes and suffixes are
   added "on the fly" by spelling check programs.

I cat'd together a bunch of "literary text" files - mostly FAQs and newbie
user intro-type files, then processed them to get the following frequency
distribution (note--I left in some of the more interesting punctuation, as
data points).  The test file was approx 1.6 Mbytes long.  Notice that it is
close, but not exactly like the ETOAINSHRDLU textbook example.  Does anyone
know what text was processed to obtain "ETOA.."?  I've heard it was the King
James Bible...

135371 e
104724 t
 89790 o
 86114 s
 85248 i
 85108 a
 82427 n
 77557 r
 46744 l
 45641 c
 42668 d
 39717 h
 39670 u
 35381 m
 26481 p
 24558 f
 23687 .
 20622 y
 20506 b
 19928 g
 15028 w
 13939 v
 12975 ,
 10180 -
  7349 k
  3971 x
  3230 "
  3170 )
  3135 :
  2964 (
  2746 q
  1879 '
  1695 j
  1039 z
   854 ?
   539 !
