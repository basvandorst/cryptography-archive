Newsgroups: sci.crypt
Path: msuinfo!agate!usenet.ins.cwru.edu!magnus.acs.ohio-state.edu!math.ohio-state.edu!cs.utexas.edu!uunet!mnemosyne.cs.du.edu!nyx10!colin
From: colin@nyx10.cs.du.edu (Colin Plumb)
Subject: Encryption mode for disk systems - comments requested
Message-ID: <1994Feb17.090926.17521@mnemosyne.cs.du.edu>
X-Disclaimer: Nyx is a public access Unix system run by the University
 	of Denver for the Denver community.  The University has neither
 	control over nor responsibility for the opinions of users.
Sender: usenet@mnemosyne.cs.du.edu (netnews admin account)
Organization: Nyx, Public Access Unix at U. of Denver Math/CS dept.
Date: Thu, 17 Feb 94 09:09:26 GMT
Lines: 55

Here's an idea I just came up with and I'd like to give it a little
kick around.  The actual application is a disk encryptor using
a hash function as an encrypt-only cipher in CFB mode.

As you know, encrypting different messages with the same key and IV
in CFB mode is dangerous.  Given C[0] and C'[0], you can form
C[0] ^ C'[0] = e(IV) ^ P[0] ^ e(IV) ^ P'[0] = P[0] ^ P'[0].

Now, when encrypting a disk, this means it's a good idea to have a
different IV per sector.  But less obviously, as the contents of a
sector change (a disk optimizer is a good way to do lots!), the same
sector can hold different messages at different points in time.  That
can be a leak, if the attacker can read the disk repeatedly during
normal operations.  (And if you were sure about the security of your
disk, you wouldn't be encrypting it, now would you?)

The solution I came up with was to checksum the plaintext plus a fixed
IV and sector number, and use the result as the IV for the encryption.
Well, actually, it was running a block-sized scrambler polynomial (such
as all kinds of telephone equipment uses) over the sector, and using
the last (cipher) block in the sector as the IV to encrypt the first
block.  This breaks up patterns (like every 8th bit clear in
uncompressed ASCII text), but also makes the IV data-dependent,
essentially eliminating the weakness.

But then it ocurred to me that with a message digest algorithm, key
scheduling takes almost no time, so it would be quite feasible to
change the key per sector.  As you generally want to minimize the
amount of data encrypted with any one key, it seemed that running the
scrambler over the key before starting in on the data (but after the
sector number!) would achieve this happy result.

Now, something with as much theory as an LFSR makes the XOR between
the keys of different sectors computable, but I think it still
makes a cryptanalyst's job easier.  The average Hamming weight is
high.  Because the key is so long, you can't find a pair of
sectors where it's low enough (say, less than 50) to be useful.

But enough horn-tooting.  As I think any would-be cryptographer should
be, I'm skeptical of the benefits of this idea.  I'd like to run it
through the collective net.wisdom to please tell me if it looks good or
not.

(Note: the scrambler polynomial is actually done 32 bits at a time,
using a lagged-add technique.  Considering it as encryption (which it
really isn't), it's C[i] = P[i] + C[i-k1] + C[i-k2].  The bottom bits
form a degree-k2 LFSR, but the higher bits get more complex.  k2 is
the block size.  4 for MD5, 5 for SHS, 8 for HAVAL, etc.)

Any ideas?  Comments are welcomed.

(I'm also working on a good overwriting pattern for hard disks, but
I have to do a bit more research before that's ready for posting.)
--
 	-Colin
