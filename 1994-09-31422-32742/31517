Path: msuinfo!agate!howland.reston.ans.net!cs.utexas.edu!uunet!illuminati.io.com!nobody
From: ritter@pentagon.io.com (ritter)
Newsgroups: sci.crypt
Subject: Re: testable really-random rngs
Date: 1 Sep 1994 20:39:41 -0500
Organization: Illuminati Online
Lines: 310
Message-ID: <345vot$6f2@pentagon.io.com>
References: <CvCpuH.n4y@dorsai.org>
NNTP-Posting-Host: pentagon.io.com


 In <CvCpuH.n4y@dorsai.org> stevalln@dorsai.org (Steve_Allen)
 writes:


>> Of course, the pulse-width idea is not sacred.  We might just as
>> well imagine sampling instantaneous amplitude, but I don't know
>> how we would get a flat distribution of amplitude probability
>> from random noise.
>
>   If, in your reverse-bias tests, you were looking at the noise on
>a scope, you'd see that the shape of the noise changes with current
>and noise amplitude.
> you plot noise density vs voltage, at max
>noise (i.e., optimum current thru device) your plot is highly skew
>to the left. On the scope the noise looks like a flame.

 Are you implying that we *can* get a flat distribution of values
 by sampling the instantaneous amplitude of noise?

 *My* point was that there exists an amplitude counterpart (dual)
 to period measurement which may actually be easier to sample.  But
 even if we compute the amplitude difference between successive
 samples, I find it unlikely that this would give us what we want.


>>  A triangle or sawtooth wave might have such
>> probability; perhaps we could use noise to vary the slope, but I
>> am still dubious that it could give us what we want:
>
>   If you use noise to vary the slope of a triangle wave you
>haven't made your noise more random.

 You got it.

 This is just an approach which conceivably might make amplitude-
 sampling a reasonable alternative to period measurement.  Or not.
 In some cases, it could be easier to sample amplitude than to
 build the hardware to measure pulse width.


>   It sounds like the ultimate test of a RNG is whether there's a
>bias toward a certain sequence.

 No.  There are virtually infinite possibilities for bias.  Samples
 can be related by position, or by particular values at particular
 positions, or particular sets of other positions or values.

 However, if we have some device which reports back a reasonable
 range of values closely related to noise action, and we get a not-
 too-bad probability distribution of those values, we know much,
 much more than we usually know about the random device.  Just
 because we cannot know about *all* possibilities for bias does
 not mean that we should not bother to test for the most obvious
 and common.  We are unlikely to be able to do that on processed
 bits.


>At first blush this is a difficult
>test to do, because huge amounts of data have to be collected in
>real-time, and analyzed.

 I don't really care about testing every possible measurement
 (unless you intend to *use* every possible pulse).  Normally, it
 should be sufficient to grab one, put it away, then grab another
 just the way you normally use the generator.  In fact we *want*
 to test it the way we expect to use it.

 The real-time aspect of the "analysis" I want involves incrementing
 a single count in an array of counts; actual analysis can occur
 after a full trial is complete.  The real-time part is very, very
 easy, provided one has the store to hold the count array.


>   How long a sequence would you like to examine? Byte sequence
>distribution analyses give some confidence-- word sequences more,
>doubleword sequences still more.

 No, that's not it.  I don't want the usual string analysis on
 already-reduced results.  I want to measure *before* that.

 I want to have the device give me samples in some reasonable range
 of values (say, 10..16 bits +/- a few) which depend as closely as
 possible on the noise.  Then I want to collect occurrence counts
 for each value in that range; the result is a sample probability
 distribution.  A number of different trials will help tell me what
 further processing is necessary.  I don't want to have to "think"
 that a heavy modulo or MD5 are "necessary" and then just use them
 because doing so feels good.  We should use them just like our
 other tools: if and when needed.

 But if the data are random, what can MD5 possibly add?  And if
 the data are not random, how would we possibly know by testing
 the results after using MD5?  This means that we have to test
 before MD5 anyway, or we might as well discard the whole thought
 of having a random device.


>   Suppose I have a 'truly-random' generator with a slight hum, or
>RFI, that passes the 32-bit statistical analysis. How could this
>generator be attacked?

 Probably such a generator *would* be "mostly" random.  But it
 *might* not have *any* "real" randomness at all, and I wonder if
 we could normally tell the difference:

 If we assume that periods are measured with some on-board clock,
 that clock may well not be related to the interference frequency.
 Similarly, the periods between samples may also be unrelated either
 to the clock or the interference.  Thus--especially after the modulo
 operations--the results might look pretty good, and yet not be
 really random at all.  One of the main issues is how to detect this
 sort of thing, since general statistical tests will not do so.
 Indeed, this is a hardware counterpart to the sort of thing one
 does mathematically to create a pseudo-random RNG with good
 statistics.

 Another issue is knowing how much modulo (range-folding) is
 necessary, so as to support a reasonable data rate.  It is not true
 that I want a super-slow noise source just because I "think" the
 result might be "more random."  Sometimes I would rather use more
 result values which might be somewhat less random.  And I do want
 a substantial data rate.


>  One test occurs to me: Periodically sample any 64 or 128 or any
>convenient large number (:-) of contiguous bits, then wait for
>matches. If you get too many matches, or they're suspiciously
>regular, raise a flag.

 This may or may not be the sort of population test I covered in my
 recent Cryptologia article.

 But if you mean to just wait for another identical string to show
 up, you not only are going to wait a while for any substantial
 string, but I claim the results will have little or no statistical
 meaning.  A valid really-random sequence may produce the same
 string twice in a row, or not again for years.  So what?  This is
 valid operation, so what can one possibly learn from observing it?

 On the other hand, a really-random generator *cannot* avoid
 producing *every* string!  Thus, if one looks at *every* possible
 population-element (for some measurable population), one can
 develop a distribution which provides serious insight to generator
 operation.  No, we don't learn everything.

 The problem with testing processed strings is that we have already
 hidden much about the really-random process itself.  It is easy to
 hide that stuff from ourselves, be we cannot know what someone else
 can pick apart.  Better than hiding it is to see it and fix it as
 best we can.  *Then* we can do some hiding, if necessary.  But we
 will know *if* it is necessary.


>This, coupled with a simple 8- or 16-bit
>distribution analysis of the random bits, should be pretty
>comprehensive.

 Tests on the modulo output cannot possibly be nearly as revealing
 as tests on the random values before such processing.  Since
 statistical tests cannot differentiate between a really-random
 generator and a pseudo-random generator, these tests cannot show
 when a really-random generator is (mis)behaving like a pseudo-
 random generator.

 If we measure the noise instead of processed results, we have a
 better chance of finding misbehavior.


>Plus, it's easy to do in real time. All your smart
>generator needs is sufficient RAM for the sequence, a pointer into
>the sequence, and some RAM to store match times. In fact, 16 or
>even 8 bits might turn out to be an optimum size for this test.
>   Here's a math challenge: what's the optimum sample size, and how
>often to change the sample?

 The optimum sample size depends upon the population, and what you
 want to do.  For the population prediction, we probably want
 something like 2..4 SQRT(expected population), with tens or
 hundreds of trials.  For the probability distribution, we
 probably want enough counts of each value to get a feel for the
 shape; say 5..20 of each value, or more, and tens of trials.

 But if the symbol size (population) is only two, we will be able
 to catch only an indistinct view of the random device itself.
 Alas, this does not at all mean that someone else will be so
 limited.


>> While
>> it can be argued that the values after modulo operations "must" be
>> random (since the bits come from a random source and the result
>> has good statistics), these are really the wrong values to measure.
>
>   I'm thinking about this. I'm not convinced. I have an idea that
>there can be a small set of statistical tests (perhaps the set I
>mentioned above) that gives assurance against all practical
>attacks.

 We can believe your idea when you can enumerate every one of
 all "practical" attacks and show that the statistical tests are
 sufficient for each.

 Alas, it is not possible (even conceptually) to enumerate attacks,
 (practical or not) because they may depend upon concepts which do
 not yet exist.


>> If we are going to understand the design, we must have as clear a
>> view of it as possible.  This means we need to validate randomness
>> *before* modulos (or hashing or other confusion) are applied.
>
>   Of course you've got to have *sufficient* randomness before
>modulos or hashing. (In fact, you said this above.) In the case of
>hashing with MD5, if I have a generator that produces a signal
>that's 10% predictable, and I feed more than 141 bits per
>iteration, my result will be hashed to 128 bits of good randomness.
>If I feed in 2000 bits per iteration, I need not trust my number
>source very far...

 First of all, if one is content to just measure results for
 "randomness," how does one differentiate a deterministic RNG from a
 really-random RNG?  A good statistical RNG will give results which
 will look just as random.  If general statistical testing is your
 criterion, why use a really-random generator at all?

 What could it possibly mean to say that a generator is "10%
 predictable"?  Could it mean predictable _given_a_particular_
 prediction_test_?  Can success on such a test have any real
 meaning?  We confront an Opponent who can use an infinite number
 of possible tests.  We *cannot* know *how* predictable the generator
 will be to them.  The best we can hope to do is to make one which
 we can test to obey the physical laws of the design, in a design
 which uses the physical laws in a way which should produce a
 random result.  Such designs apparently can be rather subtle.
 Our testing needs to be similarly subtle.


>   One specific goal in my generator design is that any bias in the
>noise source is swamped in the sampling process.

 Depending on how this is meant, I think this is a terrible goal.
 Until you can measure the bias, how can you tell whether or not
 you have swamped it?

 I am not saying that a probability distribution will tell us how
 much bias there is.  I am saying that the distribution will give
 us insight that the random device is working properly, a detail
 which could remain unnoticed after such "bias" is "swamped."


>The noise is
>essentially converted to an oscillator with a frequency span of 10
>KHz to 1 MHz.

 Noise is not an oscillator.  Noise is a signal source.


>I sample single bits at 500 Hz. To me this is the
>equivalent of feeding extra bits into the MD5 algorithm-- I trade
>time for security.

 The implication of this is that you believe that bits separated
 in time cannot be related.  But if power-supply ripple is present,
 separated bits *can* be related (indeed, if the oscillator is not
 very stable, it could sync to ripple).  Bits separated by sampling
 periods can be related by variations caused by the sampling
 process itself (some pulses and power variations may occur *only*
 when sampling, and at *exactly* the same time for every sample).
 The theory that time separation necessarily implies independence
 is false.

 If all you are doing is trading off extra bits which might be
 taken, you might well be better off to simply take the extra bits.

 All that said, I expect that adjacent pulse periods may well have
 some interdependence, since moving the position of the middle edge
 will affect both pulses.  Longer period noise may well persist for
 many samples.  One way to avoid this is to use an appropriate
 high-pass frequency for the noise, and then skip several cycles at
 the lowest frequency of interest.  This is not "added" security;
 this is the way a noise device is properly used.


>If I were sampling at 10 times the speed, 5 KHz,
>things would be much different, and I'd be *extremely* suspicious
>of my random 10KHz signal.
>   I haven't come up with specific values of how un-random my noise
>source could be, given this scheme. It would be good to know, as it
>would simplify vetting of the noise source. (Does it pass this simple
>test? OK, it's good enough.) Or I could specify in terms of
>over-randomness or robustness.
>   In fact, this could be a standard specification of any random
>source. One could say, 'I feed 1000 bits of guaranteed minimum 98%
>random data to my MD5 hash to get 128 bits out. I therefore
>guarantee a Trust Factor of xxx.'

 I am unaware of any way to measure or guarantee a minimum amount of
 "real" randomness or anything like a "Trust Factor."

 If the data are random, why use MD5?  Or, if MD5 results are good
 enough, why use a really-random source?  Just use a statistical
 RNG and MD5 it for "unpredictability" . . . then everything just
 depends on whether or not one trusts MD5.

 ---
 Terry Ritter   ritter@io.com



