Newsgroups: sci.crypt
Path: msuinfo!netnews.upenn.edu!news.amherst.edu!news.mtholyoke.edu!uhog.mit.edu!news.kei.com!yeshua.marcam.com!zip.eecs.umich.edu!newsxfer.itd.umich.edu!gatech!udel!news.sprintlink.net!dorsai.dorsai.org!stevalln
From: stevalln@dorsai.org (Steve_Allen)
Subject: re:testable really-random rngs
Message-ID: <CvCpuH.n4y@dorsai.org>
X-Newsreader: TIN [version 1.2 PL2]
Date: Tue, 30 Aug 1994 14:06:17 GMT
Lines: 140

> When we measure pulse width--provided we can read the full pulse-
> width count--we can develop a probability distribution (pulse-
> width vs. frequency of occurrence).  Then, if we have a relatively
> flat distribution, we can have more confidence that the resulting
> values are unbiased.  In design, we can detect bandwidth problems,
> and predict whether an output modulo operation will be useful.
> When the user can re-verify the good distribution in the field,
> she can have confidence that that particular hardware is designed
> well and is also continuing to work well.

   I agree 100%. My design brings the random signal out to a jack,
where direct observations and measurements can be made.  However,
the PIC I'm using currently has about 8 bytes of ram available. It
could do timings, but it sure couldn't do a damn thing with the
results.

> Of course, the pulse-width idea is not sacred.  We might just as
> well imagine sampling instantaneous amplitude, but I don't know
> how we would get a flat distribution of amplitude probability
> from random noise.

   If, in your reverse-bias tests, you were looking at the noise on
a scope, you'd see that the shape of the noise changes with current
and noise amplitude.
 you plot noise density vs voltage, at max
noise (i.e., optimum current thru device) your plot is highly skew
to the left. On the scope the noise looks like a flame.

>  A triangle or sawtooth wave might have such
> probability; perhaps we could use noise to vary the slope, but I
> am still dubious that it could give us what we want:

   If you use noise to vary the slope of a triangle wave you
haven't made your noise more random.

> If the measured probability distribution is not perfect, this at
> least defines the requirements for subsequent processing which
> must hide that imperfection in actual use.  Thus, the probability
> distribution provides a fundamental way to describe the quality of
> the noise production itself, distinct from subsequent deterministic
> processing.

>  Consequently, the real innovation is hardware which
> holds out the possibility of a deeper testing of the generator
> itself, rather than just testing resulting processed bits.

   I like this idea very much. My device gives a running display of
total 1s vs 0s. This simple test, combined with an oscope view of
the noise in various stages of processing, gives a certain amount
of confidence. The idea of a processor that could do much more
comprehensive testing and self-diagnosis is intriguing, and I may
look into it. Such a device would need lots of RAM and perhaps a
numeric coprocessor to do real-time analysis-- one of those
'PC-on-a-card' things, perhaps.
   Sounds like a patentable idea. Sell it to the government.

> Statistical tests on resulting values obviously cannot certify that
> an RNG is "really random."  Indeed, deterministic *pseudo* random
> RNG's are designed to pass such tests!
>
> Nor can statistical tests certify that an RNG is without bias; all
> each test can do is highlight a particular sort of bias.  This point
> is important, because detecting some problems may require special
> statistical tests which will not be applied.  It is this inability
> to detect real problems using ordinary tests, in fact, which leads
> to worries about power-supply hum, RFI, zener interaction, and
> various other noise-generator design issues.  If these problems
> were easy to detect, we would not have to worry that they might be
> present.

   It sounds like the ultimate test of a RNG is whether there's a
bias toward a certain sequence. At first blush this is a difficult
test to do, because huge amounts of data have to be collected in
real-time, and analyzed.
   How long a sequence would you like to examine? Byte sequence
distribution analyses give some confidence-- word sequences more,
doubleword sequences still more.  Word sequence analysis takes 131
Kbytes, if you allot 2 bytes per entry. A doubleword (32 bits)
statistical analysis will take 8.59 gigabytes of ram. It'll also
take a while to run. (Let's see-- 500 samples per each of 2^32
bits... at 1K 32-bit samples/second that's over 68 years...)
   Suppose I have a 'truly-random' generator with a slight hum, or
RFI, that passes the 32-bit statistical analysis. How could this
generator be attacked?
   One test occurs to me: Periodically sample any 64 or 128 or any
convenient large number (:-) of contiguous bits, then wait for
matches. If you get too many matches, or they're suspiciously
regular, raise a flag. This, coupled with a simple 8- or 16-bit
distribution analysis of the random bits, should be pretty
comprehensive. Plus, it's easy to do in real time. All your smart
generator needs is sufficient RAM for the sequence, a pointer into
the sequence, and some RAM to store match times. In fact, 16 or
even 8 bits might turn out to be an optimum size for this test.
   Here's a math challenge: what's the optimum sample size, and how
often to change the sample?

> While
> it can be argued that the values after modulo operations "must" be
> random (since the bits come from a random source and the result
> has good statistics), these are really the wrong values to measure.

   I'm thinking about this. I'm not convinced. I have an idea that
there can be a small set of statistical tests (perhaps the set I
mentioned above) that gives assurance against all practical
attacks.

> If we are going to understand the design, we must have as clear a
> view of it as possible.  This means we need to validate randomness
> *before* modulos (or hashing or other confusion) are applied.

   Of course you've got to have *sufficient* randomness before
modulos or hashing. (In fact, you said this above.) In the case of
hashing with MD5, if I have a generator that produces a signal
that's 10% predictable, and I feed more than 141 bits per
iteration, my result will be hashed to 128 bits of good randomness.
If I feed in 2000 bits per iteration, I need not trust my number
source very far...
   One specific goal in my generator design is that any bias in the
noise source is swamped in the sampling process. The noise is
essentially converted to an oscillator with a frequency span of 10
KHz to 1 MHz. I sample single bits at 500 Hz. To me this is the
equivalent of feeding extra bits into the MD5 algorithm-- I trade
time for security. If I were sampling at 10 times the speed, 5 KHz,
things would be much different, and I'd be *extremely* suspicious
of my random 10KHz signal.
   I haven't come up with specific values of how un-random my noise
source could be, given this scheme. It would be good to know, as it
would simplify vetting of the noise source. (Does it pass this simple
test? OK, it's good enough.) Or I could specify in terms of
over-randomness or robustness. 
   In fact, this could be a standard specification of any random
source. One could say, 'I feed 1000 bits of guaranteed minimum 98%
random data to my MD5 hash to get 128 bits out. I therefore
guarantee a Trust Factor of xxx.'

> This approach differs substantially from the concepts we have seen
> previously, and deserves considerable credit and support.

-Steve

