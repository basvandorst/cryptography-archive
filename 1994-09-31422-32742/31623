Path: msuinfo!agate!howland.reston.ans.net!math.ohio-state.edu!jussieu.fr!univ-lyon1.fr!swidir.switch.ch!newsfeed.ACO.net!Austria.EU.net!EU.net!uunet!illuminati.io.com!nobody
From: ritter@pentagon.io.com (ritter)
Newsgroups: sci.crypt
Subject: Re: testable really-random rngs
Date: 5 Sep 1994 03:09:54 -0500
Organization: Illuminati Online
Lines: 273
Message-ID: <34ejoi$r48@pentagon.io.com>
References: <CvMtr5.21t@dorsai.org>
NNTP-Posting-Host: pentagon.io.com


 In <CvMtr5.21t@dorsai.org> stevalln@dorsai.org (Steve_Allen)
 writes:


>>>   Suppose I have a 'truly-random' generator with a slight hum, or
>>>RFI, that passes the 32-bit statistical analysis. How could this
>>>generator be attacked?
>>
>> Probably such a generator *would* be "mostly" random.  But it
>> *might* not have *any* "real" randomness at all, and I wonder if
>> we could normally tell the difference:
>
>   I confess to being confused by this. If we can't tell the
>difference, what's the difference? (I assume that we can test as
>well as The Opponent.)

 First, the "randomness" tests we know are not conclusive; they
 just test for particular kinds of bias, for example:

    There are probably hundreds of tests for "randomness."  Perhaps
    the most interesting recent test (because of its claims for
    generality) is:

       Maurer, U.  1990.  A Universal Statistical Test for Random
       Bit Generators.  Advances in Cryptology--CRYPTO '90.  409-420.

    Basically, if we consider each n-bit sequence a code value, this
    test computes a statistic based on the distance between each
    occurrence of the same value in the test sequence, for each
    possible value.  The resulting statistic is said to be normally
    distributed, with expectation and variance being functions of
    code value size.

    This test has the advantage of computing a statistic which is
    related to "entropy" in the sense of coding theory, but not
    necessarily "entropy" in the sense of "unpredictability."  The
    article says: ". . . it is certainly a necessary . . . condition
    that [a pseudo-random bit generator] pass the test presented
    here."  But if a statistical RNG can pass the test, hardware
    problems which mimic the operations in such RNG's are likely to
    pass the test as well, despite being absolutely predictable.


 And of course we expect that The Opponent may well have superior
 resources and thus be able to try more possibilities than we can.


>>>  One test occurs to me: Periodically sample any 64 or 128 or any
>>>convenient large number (:-) of contiguous bits, then wait for
>>>matches. If you get too many matches, or they're suspiciously
>>>regular, raise a flag.
>>
>>[...]
>
>   Well, in your post you were saying "Distributions of final
>values are bogus because pseudo-rngs are designed to pass these
>tests." This test would catch the pseudo-random fellas, and some
>biases (power-supply ripple, etc.) in truly-random guys. It sounded
>as if you say that trngs with no bias, and prngs pass most tests.
>I suggested this test to catch the prngs.

 At least as I understand the description, this test would only
 catch those PRNG's which expose their entire state in the result,
 because those RNG's produce a particular result just once per
 cycle.  However, those PRNG's with a much-larger internal state
 will combine internal values and a particular result may occur many
 times per cycle.  In terms of hardware, a board with multiple
 timing constants, substantial gain and the possibility of feedback
 offers ample opportunity for a large physical internal state which
 could nevertheless be generally predictable.

 In any case, simply measuring the distances between occurrences
 of any one particular code value (without simultaneously measuring
 also the other code values), does not seem to me to be likely to
 be a very powerful statistic.


>> What could it possibly mean to say that a generator is "10%
>> predictable"?  Could it mean predictable _given_a_particular_
>> prediction_test_?  Can success on such a test have any real
>> meaning?  We confront an Opponent who can use an infinite number
>> of possible tests.
>
>   Of course I mean 10% predictable using all possible tests.

 There is virtually no limit to the number of biases possible in
 an RNG, nor to the number of statistical tests which would be
 required to show that an RNG is without bias.

>Sheesh. I again assume we can test as well as The Opponent.

 This is simply false.  Often we may have the compute power to
 accomplish a given test, although some may be beyond our reach.
 However, because of superior resources, The Opponent may be able
 to try far more possibilities than we can.  In any case, The
 Opponent may well have types of test which we would not conceive
 of using.


>> I am unaware of any way to measure or guarantee a minimum amount of
>> "real" randomness or anything like a "Trust Factor."
>
>   I'm surprised. If a RNG is biased, surely there's a way to
>measure the bias.

 Of course there's a way to measure the bias; just tell me what
 the bias is, and I can measure it (well, usually).  The problem
 is *finding* the bias.

 This is the old "cannot disprove a negative" problem:  There are
 far too many cases to try them all, and that is what it would
 take to prove that there is no bias.  (This is without even
 getting into the probabilistic nature of the testing itself.)


>If some bits can be predicted, how many? Is the
>only measure that of perfection/imperfection/theoretical-
>imperfection?

 Presumably, a test which indicates bias will test the ability of
 a particular type of prediction to predict.  Normally, this would
 be a fractional ability, and it would be somewhat problematic
 how that could be used.  (This knowledge could be used in a brute-
 force search, to try the most-probable possibilities first, for
 example.)  However, since we propose to have real-world hardware,
 there may be any number of real things which could go wrong which
 would be absolutely predictable.


>> If the data are random, why use MD5?  Or, if MD5 results are good
>> enough, why use a really-random source?  Just use a statistical
>> RNG and MD5 it for "unpredictability" . . . then everything just
>> depends on whether or not one trusts MD5.
>
>   First you say, "An RNG may exhibit perfect measured randomness
>yet be imperfect, yea useless."
>   Then you say, "If it's perfect, what more could you need?"

 I suppose a better description would be "predictability":  An
 RNG may exhibit good measured randomness and yet be predictable;
 LFSR's being the classic example.  And if MD5 really makes a
 sequence unpredictable, I don't see much of an advantage to
 having a "really" random sequence.

 Note the qualifying "if":  Having a really-random RNG which we can
 trust allows us to avoid that "if," as well as the computation
 overhead of MD5.  Some users may want a fairly substantial
 bandwidth.


>   RNGs are fragile. This very thread itself is all about how
>untrustworthy an apparently random source can be.

 This thread is also about how deceptive such a source can be.
 Just because a few tests come out OK does not mean that the source
 is either unpredictable *or* really-random.

 Suppose the "really-random" values actually turn out to be produced
 by some complex on-board oscillation phenomena (actually quite
 possible in a linear system with substantial gain and strong digital
 output):  If we measure close to the device, we may be able to
 see this, and then fix the hardware, or at least not use it.  But
 if those same values are then randomized with MD5 or even a strong
 modulo, we may not see the problem.  This will not mean, however,
 that the problem does not exist.


>   I only mention MD5 because it's easy to say and use. Any
>operation (modulo, etc) that adds trust to your RNG design is
>equivalent (to me, anyway).

 Yes.  I don't like MD5 because I am not aware of a general theory
 of design for it.  But I don't like *any* sort of permanent
 confusion in a really-random generator because that is sure to make
 detection of improper operation difficult.  Yet if the generator is
 operating improperly, The Opponent may be both able to detect this,
 and also make use of it.


>   It would be nice to design an idiot-proof, build-this-and-play,
>cryptographically strong RNG. One that, given variable board
>layout, component tolerances, RF shielding, etc., etc., will still
>be robust enough to guarantee cryptographically strong operation no
>matter who builds it. So that if switching noise couples into your
>amplifier, or you have a 1000-watt paging tower on your roof, you
>have enough entropy per bit that the output is random regardless.

 Yes.


>   So you start with the best random source you can get, then you
>pack on extra entropy.

 Well, no.

 First, you must allow the user of that particular board to assure
 that the random source is actually functioning--and as well as
 required--at, or just before, the time it is to be used.  If the
 random source is not working, the really-random generator should
 not be used.

 Next, I would distinguish between values based on really-random
 results and values confused by hashing or ciphering.  Presumably,
 enciphered values do have high entropy--provided one cannot break
 the cipher!

 The advantage of really-random values is that, for the foreseeable
 future anyway, nobody and no agency is likely to be able to "break"
 the physical randomness exposed in noise.  This means that we do
 not have to rely upon or "trust" the usually unknown strength of
 a cipher.

 As far as I can see, this is really the whole point of building
 a really-random device:  If we are willing to accept enciphered
 values as being sufficiently secure, we don't need much if any
 "real" randomness at all.  But if we are not so willing, no
 reasonable amount of confusion is likely to make up for a lack
 of "really" random data.


>Kinda like using 8-round IDEA even though it
>shows perfect mixing after 2 rounds.

 I'm sure there is much more to the decision to use 8 rounds in
 IDEA (PES, IPES, ...) than a measurement of "perfection" at 2
 rounds and the thought "Gee, I guess we'll be four times better
 than perfect!"  I mean, why four?  Why not twice, or eight times?
 Do we really just pick this out of a hat?  Is that the way a
 real cryptographic designer goes about all this?  Does anyone
 really think that?


>   Schneier has a whole section on this (Applied Cryptography pp.
>71,72).

 Good for him.  Does he tell us how to build a reliable really-
 random RNG?  Does he bring up the nasty reality of the limitations
 of statistical tests of real randomness?  For that matter, does he
 bring up the need to certify or validate received public keys in
 public-key designs like PGP?  Does he weigh-in on the cryptographic
 strength of PGP's "web of trust" protocol?  Hmmm.

 I'm afraid many areas of cryptography still have "old wives' tales,"
 unexamined designs and rules-of-thumb which are valid only in
 particular contexts.  And the "open literature" handbook on really-
 random cryptographic RNG's has yet to be written . . . .


>   Packing in extra entropy doesn't cripple your rng-- it makes it
>stronger against possible unforseen effects due to aging,
>environment, etc, etc.
>   No?

 No.  First of all, in this context, I prefer to reserve "entropy"
 for really-random values (that is, "unconditional" entropy).  Thus,
 I would say that enciphering or hashing random values does not
 _per_se_ add entropy, although combining a larger amount of less-
 random data into a smaller result might put the existing entropy
 in a smaller space, if that were needed.

 Next, we normally expect that the "really" random part of the
 really-random generator must function.  (If we were able to accept
 the random device not working, we would not need such a device.)
 So if the output "looks random" with the random device not
 functioning, we have a problem instead of a solution.


 ---
 Terry Ritter   ritter@io.com



