Newsgroups: comp.ai,comp.ai.nat-lang,comp.compression,sci.crypt
Path: msuinfo!agate!howland.reston.ans.net!europa.eng.gtefsd.com!news.ans.net!newsgate.watson.ibm.com!watnews.watson.ibm.com!yktnews.watson.ibm.com!rocky!lwloen
From: lwloen@rchland.vnet.ibm.com (Larry Loen)
Subject: Re: American language standardized dicti
Sender: news@rchland.ibm.com
Message-ID: <1993Dec22.190012.51064@rchland.ibm.com>
Date: Wed, 22 Dec 1993 19:00:12 GMT
Reply-To: lwloen@rchland.vnet.ibm.com
Disclaimer: This posting represents the poster's views, not necessarily those of IBM
References: several <1993Dec20.183340.8769@cs.ucla.edu>
Nntp-Posting-Host: rocky.rchland.ibm.com
Organization: IBM Rochester
Lines: 54
Xref: msuinfo comp.ai:20137 comp.ai.nat-lang:1013 comp.compression:9972 sci.crypt:22200

In article <1993Dec20.183340.8769@cs.ucla.edu>, jperry@oahu.cs.ucla.edu (John Perry) writes:
|> davesparks@delphi.com (Dave Sparks) writes:
|> >  
|> >What would seem really useful would be a long dictionary of English words,
|> >sorted according to frequency of usage.  That way, you could take a
|> >dictionary containing 128K words, for example, and easily create a smaller
|> >dictionary containing the "n" most commonly used words, and "n" could be a
|> >convenient power of two.
|> >
|> >Does such an item exist, or has it been considered?
|> >
|> 
|>   Try these:
|> 
|>   Henry Kucera and W. Nelson Francis.  Computational Analysis of Present-Day
|>   American English.  Brown University Press, Providence, 1967.
|> 
|>   Anne Zettersten.  A Word-Frequency List Based on American English Press 
|>   Reportage.  Publications of the Department of English, Vol. 6, University
|>   of Copenhagen, 1978.
|> 
|> 				happy typing :-),
|> 				John
|> 
|>

Alternatively, what attacks are you going to run, Dave?  In most 
dictionary attacks I know of, the only one where frequency matters
is trying to "crack" someone's password by trying bad choices
for passwords.  There is at least one book on "hacking" in my local
library that claims to show the most common choices (including bonehead ones
like "password" and "userid") and not so obviously bad ones like
favorite cartoon characters' names.  (If you stay away from Tolkien
and Star Trek, you'll be better off, too).

For simple subsitution, however, the larger the word list, the better.

One method I used to rapidly get my own 10K word list was to boil
down various electronic fora (sci.crypt would be a pretty good choice,
but vary the subject matter for best results,), break it into 
individual unique words via an insertion sort.  I think I uppercased 
everything, but cleverer ploys could be used to isolate proper names than 
that.  If all you want is a decent sized word list, that's an excellent 
head start.

Of course, I ran the net results through my own word processor dictionary
of about 140,000 words.  There were still a lot of trips to the dictionary,
(I used Webster's 3rd), but it sure beats typing them in one at a time.

-- 
   Larry W. Loen        |  My Opinions are decidedly my own, so please
                        |  do not attribute them to my employer

   email to:  lwloen@rchland.vnet.ibm.com
