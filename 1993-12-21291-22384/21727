Path: msuinfo!agate!howland.reston.ans.net!pipex!sunic!news.funet.fi!nntp.hut.fi!vipunen.hut.fi!nispa
From: Tapani.Lindgren@HUT.Fi
Newsgroups: comp.compression,sci.crypt
Subject: Re: Maximal Entropy Sound Compression
Date: 5 Dec 1993 00:58:03 GMT
Organization: Helsinki University of Technology
Lines: 78
Distribution: inet
Message-ID: <2drbmr$iqo@nntp.hut.fi>
References: <CH7sB1.MqC@world.std.com>
NNTP-Posting-Host: vipunen.hut.fi
Originator: nispa@vipunen.hut.fi
Xref: msuinfo comp.compression:9768 sci.crypt:21727

[Crossposted to comp.compression and sci.crypt]

kkirksey@world.std.com (Ken B Kirksey) writes:
[About using a sound board as a random number
 generator; edited]
>Someone on cypherpunks suggested encrypting
>the sampled sound data to increase it's entropy
...
>what compression algorithm will give an output
>with maximal entropy given "noise" as input?

What exactly do you mean by this question?

You cannot add entropy by a deterministic algorithm.
(You can of course reduce entropy by "throwing away"
information.)

Compression algorithms reduce redundancy.
If you are using your samples for encryption,
you should try to get rid of as much redundancy
as possible.

Sound compression algorithms are usually designed to
eliminate most of the redundancy relatively fast.  Many
algorithms are "lossy" - they throw away some
information that is deemed unimportant.

Further reducing the redundancy (while retaining most
of the information) requires complicated
modelling and expensive calculations.  In normal sound
compression this doesn't usually pay off.  Therefore
most sound compressors leave a considerable amount of
redundancy in the data stream.


If your situation, where the information in the sound
samples is not meant to be recovered, you can probably afford
to discard a good deal of information in the process.

First, you should discard all but the least significant
bit of the samples - the low bit is supposedly more
random than any of the other bits.

Then you might want to run some test on the samples to make
sure it doesn't contain any hidden patterns that my have been
introduced through interference from the board's sampling
logic or other circuitry in the computer.  Run Fourier analysis
to find any unusually strong single frequency components.
Count the successors of each bit pattern and check that the
variation of 1/0-probabilities is not too large (nor small).

If the sampled data fails any of these randomness tests, discard
it and get a new block.


There will probably be some systematic bias in the samples
(for example 51.1% 0 and 49.9% 1).
You can eliminate this small redundancy as follows:

input output
01    0
10    1
00    nothing
11    nothing

Note that this discards almost 75% of the information even
when the bias is small, and even more if the bias is greater.
And throughput is not guaranteed.  Any frequency analysis
should have been run before this step.


You also mentioned encrypting the samples.  I assume this is a
typo.  Encryption doesn't alter the entropy and redundancy of
the samlpes - it just adds one level of complication in dealing
with them.  I can't recommend this.


Tapani
