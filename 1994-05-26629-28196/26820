Newsgroups: sci.math,sci.crypt,alt.security,alt.security.pgp,alt.security.ripem,comp.security.misc,alt.privacy
Path: msuinfo!agate!howland.reston.ans.net!EU.net!sun4nl!cwi.nl!dik
From: dik@cwi.nl (Dik T. Winter)
Subject: Re: Sieve Blocking for MPQS to improve mem-hier locality & performance
Message-ID: <Cp7FpE.C7t@cwi.nl>
Sender: news@cwi.nl (The Daily Dross)
Nntp-Posting-Host: boring.cwi.nl
Organization: CWI, Amsterdam
References: <2q3hjp$pqc@linus.mitre.org> <2q40b1INN6q2@life.ai.mit.edu> <2q40pkINN6vr@life.ai.mit.edu>
Date: Tue, 3 May 1994 02:17:37 GMT
Lines: 46
Xref: msuinfo sci.math:71101 sci.crypt:26820 alt.security:16312 alt.security.pgp:12383 alt.security.ripem:825 comp.security.misc:9676 alt.privacy:14908

In article <2q40pkINN6vr@life.ai.mit.edu> lethin@kiwi.ai.mit.edu (Rich Lethin) writes:
 > For MPQS (the RSA129 sieve) it seems like one could write the inner loop to
 > walk different factor-base strides through the sieve concurrently,
 > "blocking" the accesses with more steps for the smaller strides and fewer
 > steps for the larger strides to keep them "in-sync" and spatially local in
 > the cache.  The algorithm would be cache-bandwidth-limited rather than
 > main-memory-limited.

I do not think that walking multiple strides from different processors is
really feasable.  I may also not here that most modern variants omit the
small primes in the sieving phase, allowing for their presence in a "fudge"
factor, to be dealt with later.  This means that all strides are reasonably
large.  Blocking does not help very much here.  There are quite a few
calculations to perform to determine where to start in your sieving array,
but once you know the starting point sieving is easy.  Blocking would mean
multiple calculations of the starting point, which would make the overhead
for those calculations more prominent.
 > 
 > This would be analogous to the cache-blocking techniques that people use to
 > get better LINPACK performance on machines with caches.

The difference with blocked matrix techniques is that those allow quite a few
calculations to be performed (calculations you have to do anyhow) on small
blocks of the matrices.
...
 > Could this technique be used to try to compensate for the big disparity for
 > disk bandwidth for random seeks and sequential accesses?  If memory speed
 > is ~500Mbits/sec (RAMBUS) and disk IO ~10MBytes/sec (SCSI) the ratio of
 > bandwidths is only 6.25, but the cost of memory is about 10x that of disk.
 > So it might be more effective to do the sieving via the disk.
 > 
It might be, from a commercial perspective.  Expect a slow down by just
the factor in access speeds.  Moreover, and this is more essential, you
only covered bandwith, not latency.  Disk latency will kill it, even if
you are using 8k disk blocks you may expect at some early stage that each
access is an access to a different disk block, and than you ought to count
the net effect of pulling in a single disk block to do an update to a
single element.

The algorithm can be done so that more than a single stride are handled
at the same time, but I really doubt that will compensate for it.

In summary, I think this is a dead alley.
-- 
dik t. winter, cwi, kruislaan 413, 1098 sj  amsterdam, nederland, +31205924098
home: bovenover 215, 1025 jn  amsterdam, nederland; e-mail: dik@cwi.nl
