Newsgroups: sci.math,sci.crypt,alt.security,alt.security.pgp,alt.security.ripem,comp.security.misc,alt.privacy
Path: msuinfo!agate!news.ossi.com!ihnp4.ucsd.edu!library.ucla.edu!csulb.edu!csus.edu!netcom.com!strnlght
From: strnlght@netcom.com (David Sternlight)
Subject: Re: RSA-129
Message-ID: <strnlghtCp7KJ4.2w0@netcom.com>
Reply-To: david@sternlight.com (David Sternlight)
Organization: DSI/USCRPAC
References: <WARLORD.94Apr27000625@incommunicado.mit.edu> <2q3huh$a99@sol.tis.com> <strnlghtCp73J9.G8B@netcom.com> <2q4256$ahr@earendel.umiacs.umd.edu>
Date: Tue, 3 May 1994 04:01:51 GMT
Lines: 36
Xref: msuinfo sci.math:71105 sci.crypt:26825 alt.security:16316 alt.security.pgp:12391 alt.security.ripem:830 comp.security.misc:9680 alt.privacy:14914

In article <2q4256$ahr@earendel.umiacs.umd.edu>,
David Harwood <harwood@umiacs.umd.edu> wrote:
>In article <strnlghtCp73J9.G8B@netcom.com>,
>David Sternlight <david@sternlight.com> wrote:
>[...]
>|No. My point is to get qualified experts here to state specific and
>|up-to-date security properties of RSA keys of length 512 and 1024, in terms
>|of time to crack by organizations with the presumed power, money,
>|computation power, and motivation of the NSA as a surrogate for its foreign
>|opposite numbers. It is also to get some similar measure on how long the
>|IPRA key should be.
>\\\\\\
>I think you already have enough information in several postings
>to make as good a guess as anyone - something like 50-200 years
>to factor a 1024 bit RSA number using best feasible technology and
>algorithms - say 10,000 processors of custom design (for the particular
>factoring algorithm).

I'm seeing assertions (and without an analysis; that's what your response
is), ranging all over the map. I'm still waiting for an analysis. Right now
I've seen credible-sounding estimates for 1024 bit keys ranging from your
50-200 years on the high side, to 1-2 years on the low side. It's not good
enough for those of us with valuable data to protect. We need more than what
seem to be intuitive guesses. The Scientific American article had guesses,
too, and they were off by orders of magnitude.

> Only significantly more efficient algorithms
>are going to improve the situation. There has been no analysis of
>parallel computational complexity since no one has tried to specify
>a feasible model of a custom processor (eg circuit complexity, parallel
>memory access, or. say. constant-time evaluation of polynomials of
>certain "size".) No one can know how long it would take to factor
>a long number unless they specify a certain hypothetical processor
>(with its very large cache) and a multiprocessor model for the problem.

I did so with my benchmark assumptions post.
