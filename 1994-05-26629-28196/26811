Path: msuinfo!agate!howland.reston.ans.net!usc!bloom-beacon.mit.edu!ai-lab!kiwi!lethin
From: lethin@kiwi.ai.mit.edu (Rich Lethin)
Newsgroups: sci.math,sci.crypt,alt.security,alt.security.pgp,alt.security.ripem,comp.security.misc,comp.lsi
Subject: A fast sieving machine does it *IN* the DRAMs.
Date: 2 May 1994 23:55:04 GMT
Organization: MIT Artificial Intelligence Lab
Lines: 60
Message-ID: <2q43soINN906@life.ai.mit.edu>
References: <WARLORD.94Apr27000625@incommunicado.mit.edu> <strnlghtCoxF2o.DLA@netcom.com> <2pova8$mf3@fido.asd.sgi.com> <2pp12d$he6@hermes.unt.edu> <2prdckINNbge@early-bird.think.com> <1994Apr23.210009.5796@muvms6> <2pp0f0$mf3@fido.asd.sgi.com> <2pu8bn$dpe@linus.mitre.org> <strnlghtCp3zu5.5C7@netcom.com> <strnlghtCp40CK.684@netcom.com> <Cp598L.Fy9@cwi.nl> <2q3hjp$pqc@linus.mitre.org> <2q412qINN735@life.ai.mit.edu>
NNTP-Posting-Host: kiwi.ai.mit.edu
Xref: msuinfo sci.math:71090 sci.crypt:26811 alt.security:16302 alt.security.pgp:12365 alt.security.ripem:818 comp.security.misc:9667 comp.lsi:3923

A very rough back-of-the-envelope calculations for designing a VLSI custom
MPQS sieving machine for large factoring problems follows.  The inner loop
of the MPQS sieve that was used in the large email-coordinated factoring
effort used to crack the single RSA129 public-key challenge was basically a
repeated large-prime-stride single-precision "vector+constant" calculation.
Each workstation ran this over 20Mbyte section of the number space that was
chosen by an assigned random seed.  Performance is primarily limited by
main memory bandwidth.

All numbers are rough guestimates.

On-chip bandwidth for DRAMs can be much higher than off-chip bandwidth for
various reasons (no pinout constraints of chip crossings).  A 4Mbit 80ns
DRAM can read a 2048-bit row every 40 ns.  That results in an on-DRAM
bandwidth of 6.4 Gbytes/sec.  

MPQS would have to have perfect spatial locality to use all of that
6.4Gbytes/sec, since the DRAM reads large rows at a time.  An algorithmic
modification might do it, but you might also just replicate the DRAM's row
decoders to read 32 independent 32-bit numbers.

Give away 1/2 the chip for the floating point units, so you only get 2
Mbits or storage on that chip.

(Hand-waving argument ignores logic/memory process incompatibilities, power
distribution and dissipation problems from using all of the DRAM at one,
the ability to build an on-chip FP unit that fast, etc.)

The chip, mass manufactured, "costs" the same as a regular 4 Mbit DRAM
($12).

Use 128 of these chips to build a 32 Mbyte machine, which costs about $1500
for the chips, maybe another $1500 for packaging and cooling.

A "normal" CPU these days might read 4 words every 200 ns, and only use one
of them (due to bad spatial locality) so it sieves at only 20 Mbytes/sec
max.  With some vector memory references and prefetching maybe it could get
up to 80 Mbytes/sec.  The bottom line.

Smart Memory Machine:
Cost:          $3000  (cost, not price)
Sieving Speed:  6400 Mbytes/sec

Workstations:
Cost: 	       $3000
Sieving Speed:    80 Mbytes/sec

eg. a custom smart memory machine could get about 80x to 320x better price
performance than workstations using similar VLSI technology.  Again, the
key is avoiding the chip-crossings.

Of course, the email cycles used to crack RSA129 were "free".

The thing that's really interesting is trying to leverage knowledge of VLSI
partitioning and the algorithmic possibilities to design custom machines.
That's where the REALLY big wins come in.  Examples: the Quickturn
simulation hardware (using FPGAs directly) for doing logic simulation and
EXA add-on boards for doing hydrodynamic simulations using lattice-gas
algorithms blow away their workstation-based counterparts.

