Path: msuinfo!agate!library.ucla.edu!europa.eng.gtefsd.com!news.umbc.edu!olson
From: olson@umbc.edu (Bryan G. Olson; CMSC (G))
Newsgroups: sci.crypt
Subject: Re: Random mappings vs. random permutations (*bonk*)
Date: 22 May 1994 20:55:56 GMT
Organization: University of Maryland, Baltimore County
Lines: 68
Message-ID: <2rogss$63a@news.umbc.edu>
References: <16FBAF7A6S86.C445585@mizzou1.missouri.edu> <EACHUS.94May19113747@spectre.mitre.org>
NNTP-Posting-Host: umbc7.umbc.edu
X-Newsreader: TIN [version 1.2 PL2]

Robert I. Eachus (eachus@spectre.mitre.org) wrote:
: In article <16FBAF7A6S86.C445585@mizzou1.missouri.edu> C445585@mizzou1.missouri.edu writes:

:    What is the expected number of cycles?  Pick a block at random.
: Follow the chain until you have a cycle.  This (first) cycle has an
: expected length of sum for k equal 1 to 2^n of k*2^-n.  This first
: chain has an expected length of (2^n - 1)/2.  But then we have to
: compute the expected length of some other chain by choosing a starting
: block not in the first chain, and so on until we have accounted for
: all blocks.

: What you are really looking for is the expected
: length of the chain which includes a block chosen at random.  This is
: the same as the length of the first chain above: (2^n - 1)/2.  But
: that is STILL not the whole story.  What is the probability that the
: IV and key you choose generate a chain shorter than the message to be
: encoded?  

Here's the exact distribution:

Given a set S of N elements (here N=2**n), an element x of S, and a
permutation P which is randomly selected from a uniform distribution
of all permutations on S, the probability that x is in a cycle of
length m is:

     1/N   if 1<=m<=N
      0    otherwise

(I gave a proof in another thread.)

All possible cycle lengths are equally likely.  A corollary is that
the expected cycle length is (N+1)/2, and the probability that the
cycle containing x if of length m or less is m/N (for 0<=m<=N).

:   >    I haven't gone and looked up the formula for the expected value
:   > of i before I hit a cycle (all my prob/stat books are at home),
:   > but I think it will follow a hypergeometric distribution.  2**63
:   > looks reasonable, though.

It's much simpler than hypergeometric --  it's uniform.

:   >     Unless I'm missing something else, if E() were a random *mapping*,
:   > we would expect 2**32 cycles.  It looks to me like feeding back one bit
:   > at a time, or even 32 bits at a time, would approximate a random mapping,
:   > and that it would lead to an expected cycle length of 2**32 or so.  I'm
:   > interested in hearing whether or not I'm missing something else here....
:   > (This may be where I read about that 2**32 cycle length--I know I read it
:   > (or misread it) somewhere.)

The (N+1)/2 figure for random permutations is exact, while the the
2**32 (or square root of N) figure for random functions is only "on
the order of...".  The way output feedback mode is defined, you can
feed back less than the full 64 bit block which would usually be a big
mistake.

:   >    One other point: Even though we can talk about expected cycle
:   > lengths, we can always get unlucky and hit a really short cycle
:   > length.  And with any block cipher with keys that self-decrypt
:   > (ie, E(E(x,k0),k0) = x), we'll always get stuck in length 2
:   > cycles--E(a,k0) = b, E(b,k0) = a, ...

Since the probability that the cycle containing x is of length m or
shorter (0<=m<=N) is m/N, this is normally not a big concern.  If an
adversary is searching for one of N keys, the chance he'll find it in m
or fewer tries is also m/N (0<=m<=N), so we're committed to trusting
such probabilities anyway.

--Bryan Olson
