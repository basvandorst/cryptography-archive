Path: msuinfo!agate!howland.reston.ans.net!wupost!waikato!auckland.ac.nz!pgut01.cs.aukuni.ac.nz
From: pgut01@cs.aukuni.ac.nz (Peter Gutmann)
Newsgroups: sci.crypt
Subject: Re: Encryption Using Huffman Coding
Date: 2 Apr 1994 14:29:56 GMT
Organization: University of Auckland
Lines: 47
Message-ID: <2njvh4$rdp@ccu2.auckland.ac.nz>
References: <CnKDnt.3vr@chinet.chinet.com>
NNTP-Posting-Host: cs13.cs.aukuni.ac.nz
X-Newsreader: NN version 6.5.0 #7 (NOV)

schneier@chinet.chinet.com (Bruce Schneier) writes:

>If you take a data file and compress it using Huffman coding techniques,
>you get something that looks random.  Make the coding table the key, and
>the compressed text is now ciphertext.  Although there is no reason to
>believe that this is in any secure, I know of no one who has any idea how
>to cryptanalyze it.

>I have heard a lot of folklore on this topic; does anyone know of any
>actual reserach results?

I looked at it a few years ago using arithmetic coding, wrote up a few pages 
on it, posted it either to sci.crypt or comp.compression, and have never 
been able to find it again since then.  From what I remember, the end result 
was rather discouraging.  Assuming an order-0 model, you needed to generate 
some hundreds of random symbols to obscure the initial probabilities of each 
symbol being compressed.  When compressing a Markov source, you could make 
an estimate of what symbols were likely to occur, and use this as the basis 
for an attack.
 
To translate this into plain English:  Let say you want to "encrypt" text by 
compressing it using a model in which the symbol probabilites have been 
skewed by seeding the model based on some key.  In the worst case, your key 
could affect only symbols which don't appear in the text, resulting in no 
"encryption" at all.  For a random key and English text, I think you needed 
something like 50-odd random symbols before there was a noticeable effect on 
the compressed data (this was a while back, YMMV on the exact number).
                                   
With a higher-order model, the problem is even worse - the skewing of 
probabilities is even less effective since most of the time you change the 
probabilities of symbols which *never* occur (this counts for most 
dictionary-based compressors as well, which are roughly equivalent to 
higher-order Markov models in an obscure way I won't bore you with here).
 
There are other problems as well, such as the amount of compression achieved 
being a good indication of how well the initial seeding of the model fits 
the data being compressed - this leaks information on the key being used.
 
I have a (possibly unpublished) 1992 paper here entitled "A Chosen-plaintext 
Attack on an Arithmetic Coding Compression Algorithm" by some people from 
the Queensland University of Technology (Australia) which, although not as 
pessimistic as my analysis, tends to come to the same conclusions.
 
In short, it's not worth it.  Compress the data first, then use a proper 
encryption algorithm to encrypt it.
 
Peter.
