Newsgroups: sci.crypt
Path: msuinfo!agate!howland.reston.ans.net!cs.utexas.edu!uunet!mnemosyne.cs.du.edu!nyx10!colin
From: colin@nyx10.cs.du.edu (Colin Plumb)
Subject: Re: Encryption & Compression
Message-ID: <1994Apr26.085911.22146@mnemosyne.cs.du.edu>
X-Disclaimer: Nyx is a public access Unix system run by the University
 	of Denver for the Denver community.  The University has neither
 	control over nor responsibility for the opinions of users.
Sender: usenet@mnemosyne.cs.du.edu (netnews admin account)
Organization: Nyx, Public Access Unix at U. of Denver Math/CS dept.
References: <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com> <2p188q$1e8m@locutus.rchland.ibm.com> <CHAR-LEZ-200494114739@dnwmac194.globalvillag.com> <straitsCoLMy4.F6D@netcom.com>
Date: Tue, 26 Apr 94 08:59:11 GMT
Lines: 24

In article <straitsCoLMy4.F6D@netcom.com>,
Stewart C. Strait <straits@netcom.com> wrote:
>BTW, if someone could state a real theorem that the average compression
>is zero it would be a help. I get mixed up trying to deal with input
>files of different length. The information that EOF has been reached
>probably has something to do with it, but using an infinitely long
>stream of bits seems crude and promises to make the practical results
>fuzzy.

You're right, EOF complicates things.  If you know your inputs are always
a certain length, you can encode information in the length of the output.
(As a trivial example, leave off trailing zero bits.  Voila, compression
on the average of toally random data.)

But if you include all shorter files as well, then you're stuck.
There are 2^n - 1 bit strings of length less than n.  Any compression
algorithm that does not lose information must map each of them to a distinct
bit string.  Any collection of bit strings which numbers 2^n - 1, and
includes any bit strings of length n or more must have a larger average
length.  Thus, the best you can do is to shuffle the bit strings among
themselves.  This achieves no compression, on average.
-- 
	-Colin

