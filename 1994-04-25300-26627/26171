Path: msuinfo!netnews.upenn.edu!news.amherst.edu!news.mtholyoke.edu!news.byu.edu!news.kei.com!MathWorks.Com!europa.eng.gtefsd.com!howland.reston.ans.net!pipex!uunet!gatekeeper.us.oracle.com!barrnet.net!globalvillag.com!dnwmac194.globalvillag.com!user
From: CHAR-LEZ (Char-Lez Braden)
Newsgroups: sci.crypt
Subject: Encryption & Compression
Followup-To: sci.crypt
Date: Tue, 19 Apr 1994 08:52:16 -0800
Organization: Global Village communication, Inc.
Lines: 31
Message-ID: <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>
NNTP-Posting-Host: dnwmac194.globalvillag.com

Help me understand something about these technologies, they seem related.

As I understand it, a perfectly destributed* random set of data held in a
file, is compressable.

ALSO, the goal of an encryption algorithym is to take a set of data that
has some organization to it, and obscure that organization beyond all
recgnition to those who do not have authorization.

Are either of these points wrong?

Why then, if I take a file, compress it, then encrypt it and compress it
again, it does not compress more?  This says to me that the compression
routine is still finding some inate order to the set of data.

I've gone to no small trouble to demonstraight that this is the case.

I've even tried One-Time Pad encryption routines, with the same results. 
That bothers me too, does this mean that data+Chaos<>Chaos?  Once in order,
always in order?!?

Am I missing something fundamental?

One of my Comp instructors brought up the "law of entropy" (Of which I know
nothing) and said that a bit can only hold so much data.  I can accept
that, but I cannot explain it.  My brain hurts.

*No factor shows up more than any other.

Thanks,
Char-Lez
