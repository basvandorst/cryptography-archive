Newsgroups: sci.crypt
Path: msuinfo!agate!ihnp4.ucsd.edu!swrinde!cs.utexas.edu!uunet!mnemosyne.cs.du.edu!nyx10!colin
From: colin@nyx10.cs.du.edu (Colin Plumb)
Subject: Re: Encryption Using Huffman Coding
Message-ID: <1994Apr1.190844.8319@mnemosyne.cs.du.edu>
X-Disclaimer: Nyx is a public access Unix system run by the University
 	of Denver for the Denver community.  The University has neither
 	control over nor responsibility for the opinions of users.
Sender: usenet@mnemosyne.cs.du.edu (netnews admin account)
Organization: Nyx, Public Access Unix at U. of Denver Math/CS dept.
References: <CnKDnt.3vr@chinet.chinet.com>
Date: Fri, 1 Apr 94 19:08:44 GMT
Lines: 49

In article <CnKDnt.3vr@chinet.chinet.com>,
Bruce Schneier <schneier@chinet.chinet.com> wrote:
>If you take a data file and compress it using Huffman coding techniques,
>you get something that looks random.  Make the coding table the key, and
>the compressed text is now ciphertext.  Although there is no reason to
>believe that this is in any secure, I know of no one who has any idea how
>to cryptanalyze it.
>
>I have heard a lot of folklore on this topic; does anyone know of any
>actual reserach results?

I don't know of research results, but here's how to cryptanalyze it...
This is just a monoalphabetic substitution with the additional problem that
you don't know where the symbol boundaries are.  You do, however, have
some vague idea of how long they are (e.g. between 4 and 12 bits for
8-bit bytes).  If the tree is constructed as a Huffman tree and only the
code words are assigned randomly (instead of something collon like
alll 0's being the shortest code and all 1's being the longest), then
you have an even better idea of the lengths of various symbols based on
some frequency information.

In any case, you need some likely pattern words.  Words with repeated
letters.  bomb.  attack.  retreat.  You look for repeated substrings
in the ciphertext that match the given patterns.  E.g. the ABBA pattern in
attack.  The ABCABDC pattern in retreat.  Then you start guessing other
words based on the known letters and your estimate of the number of letters
in the gaps.  Check your guesses by also guessing the inter-symbol
boundaries and seeing if the new digraphs that are formed are plausible.

Basically, it's more of a pain in the ass, but it's not that bad.

I saw another similar proposal that used semi-splay trees.  You may
remember that splay trees are a self-organizing data structure that
arrange for the most recently accessed node to be at the root.

Semi-splay trees only shift the node closer to the root (by a factor
of 2), and I think they don't preserve sorted order (not needed for
a binary encoding tree), but the operation is faster.  You define
an initial state for the tree (probably by starting from a canonical
state and encoding some prefix key with it, then discarding the
encoded prefix), and then run your message through.

You get adaptive compression (not a bad thing), and a bit of a trickier
problem to solve because symbols keep changing their encoding and length.

Perhaps I ought to try this technique along with the LZ77 algorithm as
used in ZIP and see what I get.
-- 
	-Colin
