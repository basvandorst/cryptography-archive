Path: msuinfo!uwm.edu!vixen.cso.uiuc.edu!howland.reston.ans.net!news.ans.net!hp81.prod.aol.net!search01.news.aol.com!not-for-mail
From: declipse@aol.com (DEclipse)
Newsgroups: sci.crypt
Subject: Re: Encryption & Compression
Date: 19 Apr 1994 17:13:03 -0400
Organization: America Online, Inc. (1-800-827-6364)
Lines: 30
Sender: news@search01.news.aol.com
Message-ID: <2p1hgv$rb3@search01.news.aol.com>
References: <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>
NNTP-Posting-Host: search01.news.aol.com

In article <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>, CHAR-LEZ
(Char-Lez Braden) writes:
>Help me understand something about these technologies, they seem related.
>
>As I understand it, a perfectly destributed* random set of data held in a
>file, is compressable.

Either I'm misunderstanding what you're saying or you're just plain wrong. A
perfectly distributed random set of data would NOT be compressable. How can you
compress data with no pattern to it?

>ALSO, the goal of an encryption algorithym is to take a set of data that
>has some organization to it, and obscure that organization beyond all
>recgnition to those who do not have authorization.
>
>Are either of these points wrong?
>
>Why then, if I take a file, compress it, then encrypt it and compress it
>again, it does not compress more?  This says to me that the compression
>routine is still finding some inate order to the set of data.

If you take a file and compress it, then encryt it, you would expect that it
would not be compressable again. The compression routine doesn't find any
patterns to compress since the data's encrypted.

<Other stuff deleted>
>Thanks,
>Char-Lez

Howard Fukuda
