Path: msuinfo!netnews.upenn.edu!dsinc!ub!news.kei.com!MathWorks.Com!news.duke.edu!news-feed-1.peachnet.edu!emory!swrinde!news.uh.edu!ccsvax.sfasu.edu!hoz.telescan.com!RICK
Newsgroups: sci.crypt
Subject: compression & security
Message-ID: <RICK.1.00121813@telescan.com>
From: RICK@telescan.com (Rick F. Hoselton)
Date: Wed, 20 Apr 1994 18:05:31
Organization: Telescan
Keywords: compression, Huffman, Arithmetic coding
Nntp-Posting-Host: 198.67.14.194
X-Newsreader: Trumpet for Windows [Version 1.0 Rev A]Lines: 107
Lines: 107

Much of this is in the FAQ, but I have some views
I would like to summarize and share.

Compression reduces encryption time, or it can allow a 
more complicated encryption to be performed in the 
same amount of time.

Compression reduces the amount of cipher-text available to 
the cryptanalyst.  

Compression can make "known plain-text" attacks less 
effective.  If the cryptanalyst knows a piece of plain-text 
is likely, the compression algorithm may also know it is 
likely, and generate a correspondingly short (harder to find,
more likely to be generated by accident, etc.) compression 
for it.  Knowing part of the uncompressed plain-text in a 
message may not be much help to the cryptanalyst searching for 
compressed plain-text, especially in an adaptive compressor, 
and especially when compressed symbols are spread over
non-integer numbers of bits, as in arithmetic compression.

Compression could make a chosen plain-text more difficult. 
(I think).  Generating plain-text that would compress to 
useful "chosen plain-text" in context, ( again, especially 
by an adaptive compressor) sounds tricky to me.

The cryptanalyst uses guesses about the frequency and 
arrangement of the characters in the plain-text to try to 
break the cipher.  Data compression uses its own guesses 
about these things to compress the plain-text.  If the 
compressor's guesses are good, the compressed-text will be 
"close to random".  When any "close to random" data is 
decompressed, it will yield text that has approximately 
the frequency and arrangement of characters that the 
compressor originally guessed. 

This makes the cryptanalyst's job much much more 
difficult.  The task is no longer to hunt for a 
transformation from the cipher-text into plain-text (for 
instance, English).  The task is to transform random-
looking cipher-text into random-looking compressed-text 
that can be decompressed to look like plain-text.  But, 
nearly all transformations will decompress into something 
like plain-text!  And, nearly all attempts at decoding 
appear to be (almost) successful.  It would require more 
resourses to identify each failed decryption attempt. 

I once designed a second order Huffman compression scheme 
especially for some technical English text.  When I used 
it to decompress "random" input data, the output looked 
much like technical English!  Lots of short real words, 
some long ones, and some very pronounceable nonsense 
words, with realistic suffixes and prefixes.  Later on, I 
helped some programmers to rewrite the decompressor in 
another programming language.  The text output from their 
decompressor looked almost like English, but didn't make 
any sense. They thought maybe some of the text was getting 
scrambled before compression. Actually, there was this 
little bug, hehe, a single character was decompressed 
incorrectly.  The rest of the decompressed text was an 
almost-English jabberwocky that was totally unrelated to the 
original message! 

Many real ciphers have been broken by discovering some
idiosyncracy in the format of messages.  Spaces between
words, addresses at the beginning, signatures at the end,
capitalization, cipher clerks' personal vocabularies, 
and many more have provided the fatal clues. Yet these
things also provide valuable information to the legitimate
recipients.  Data compression could (mostly) remove these 
patterns, yet allow the data to still be transmitted.

To greatly enhance security, the compression doesn't need 
to be perfect. It's guesses only need to be  about as good 
as the cryptanalyst's guesses! Any tool that a 
cryptanalyst can use to find a pattern can be used by the 
compressor to remove that very pattern.  For example, 
arithmetic compression based on single symbol frequencies 
(0-th order prediction) makes single-symbol frequency 
analysis (nearly?) ineffective. First and second order 
prediction models make digram and trigram analysis 
similarly ineffective.  Adaptive models reduce the 
idiocyncracies among different users and messages types. 
If you can recognize some non-uniformity in the compressed 
message, you can use that fact to compress the compressed 
code even further.  

Compressing cipher-text may be silly.  But to be REALLY 
silly, one might want to decompress cipher-text!  Using a 
particular model to decompress "nearly random" input (such 
as cipher-text) can generate "made-to-order" frequency 
distributions.  Do you want your cipher-text to look 
(almost) like German plain-text?  Would you like your 
encrypted images to look (almost) like English?  

Ciphers hide patterns, compression eliminates patterns.  
Just like ciphers, no compression method is perfect, but 
some are very very good. Compression can rob the 
cryptanalyst of his tools, before the message is sent.  
Compression does not make a bad encryption scheme into a 
good one, but it can enhance security on many practical 
and theoretical levels. 

These opinions are mine alone: I stole most of them 
personally. 


