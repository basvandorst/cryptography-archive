Newsgroups: sci.crypt
Path: msuinfo!uwm.edu!cs.utexas.edu!uunet!mnemosyne.cs.du.edu!nyx10!colin
From: colin@nyx10.cs.du.edu (Colin Plumb)
Subject: Re: Encryption & Compression
Message-ID: <1994Apr20.105334.29156@mnemosyne.cs.du.edu>
X-Disclaimer: Nyx is a public access Unix system run by the University
 	of Denver for the Denver community.  The University has neither
 	control over nor responsibility for the opinions of users.
Sender: usenet@mnemosyne.cs.du.edu (netnews admin account)
Organization: Nyx, Public Access Unix at U. of Denver Math/CS dept.
References: <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>
Date: Wed, 20 Apr 94 10:53:34 GMT
Lines: 60

In article <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>,
Char-Lez Braden <CHAR-LEZ> wrote:
>Help me understand something about these technologies, they seem related.

Yes.  The conenction is a field called Information Theory.  Claude Shannon
opened the field and produced most of the basic theorems.

>As I understand it, a perfectly destributed* random set of data held in a
>file, is compressable.
>
>*No factor shows up more than any other.

I assume you mean what is closer to the truth, a perfectly distributed random
set of data is *not* compressible.

Well, just "perfectly distributed" does not do it.  A "Normal" sequence
is one in which every subsequence shows up with equal probability.
b^-k for k-symbol strings if the alphabet is of size b.

Well, the string 11011100101110111100001001101010111100110111101111...,
which is perhaps more easily read as 1 10 11 100 101 110 111 1000 1001 1010...
is normal if it is infinitely expanded.  However, it is compressible into
a finite-sized generation rule.

Now, something totally random is essentially definiable as incompresible.
Note that "random" means the average ofver a number of strings; one
possible (though highly improbable) random string is all zeros.
In practice, essentially all random strings have average behaviour
relative to any desired metric, but there are always exceptions.

But that kind of circular definition of totally random perhaps
isn't what you were looking for.

>ALSO, the goal of an encryption algorithym is to take a set of data that
>has some organization to it, and obscure that organization beyond all
>recgnition to those who do not have authorization.

Well, pretty close.  If I'm encrypting some ciphertext that someone
else sent to me, the organization is already pretty obscure.  Basically
the goal is to deny any information about the data to anyone without
he key.
>
>Are either of these points wrong?
>
>Why then, if I take a file, compress it, then encrypt it and compress it
>again, it does not compress more?  This says to me that the compression
>routine is still finding some inate order to the set of data.

Why does it not compress more?  Because random, unpatterned data is
incompressible, and ciphertext generally looks unpatterned.  (If it's
a good cipher, it does.)

>One of my Comp instructors brought up the "law of entropy" (Of which I know
>nothing) and said that a bit can only hold so much data.  I can accept
>that, but I cannot explain it.  My brain hurts.

Read up on information theory.  That's what you're stumbling into, and
there are some lights to guide you in the library that may be useful.
--
	-Colin
