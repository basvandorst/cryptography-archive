Newsgroups: sci.crypt
Path: msuinfo!agate!darkstar.UCSC.EDU!news.hal.COM!olivea!charnel!charnel.net.csuchico.edu!nic-nac.CSU.net!usc!cs.utexas.edu!howland.reston.ans.net!EU.net!dkuug!uts!news.uni-c.dk!id.dth.dk!id.dth.dk!sv
From: sv@idfs3.uucp (Stig Valentini)
Subject: Re: Quick crypt of compressed data
Message-ID: <sv.766667566@id.dth.dk>
References: <sv.766048428@id.dth.dk> <Co4I5v.Lvq@ulysses.homer.att.com> <sv.766220553@id.dth.dk> <sv.766309898@id.dth.dk> <straitsCoCEpp.J74@netcom.com>
Date: Mon, 18 Apr 1994 11:12:46 GMT
Lines: 82

straits@netcom.com (Stewart C. Strait) writes:

>Stig Valentini (sv@idfs3.uucp) wrote [in part]:
>: If the distribution of compressed data isn't close to random, then the
>: compression algorithm cannot be considered optimal - because by removing the
>: remaining redundance it should be possible to compress the data
>: further.

>The vital point that seems to be overlooked here is that _all_ compression
>algorithms used on large volumes of text are _highly_ nonoptimal.

Well, thats why ZIP/ARJ chunck up large volumes and does static huffman
on each part. Generally the compression performance depends on your
input data. If you've goT a specialized compression algorithm tuned in on the 
right input data, then the compression performance can be good - no
matter what the input data iS. 

low-order statistics like single-byte frequencies are easy to predict
>statically in some applications, and usually easy to adapt to, but
>things like subject matter, favorite vocabulary, style, etc. cause long
>strings to be repeated or repeated with slight modifications.
>No one has enough memory to store a good enough probability model,
>nor enough data to get the model close enough, either adaptively or
>in advance. 

Another reason for chuncking up your input data.


>:   Time to get down to a conclusion:
>:   In the case of the Huffman tree, then it's actually quite difficult to
>: decode Huffman-encoded data without information contained in the de-coding tree.
>: And it's probably close to impossible to decode arithmic compressed data
>: without the original 'model'. So for most purposes it's sufficient only to
>: compress the compression 'model' and not the actual compression-encoded data.

>IMO, this is dead wrong. I suspect that there are several people
>on this newsgroup who have broken Huffman or arithmetic codes without

Well, I've been dead wrong before. But I still think it's hard to decode
huffman compressed data... Provided that we're talking about a
non-deterministic compression algorithm. Suppose you have the string
'AABC'. A normal Huf. implementation would perhaps assign a bitcode of '0' to
'A' - and do that always - thats the deterministic approach. If you look
at the originaL Huf. description you see that's there more than one
optimal tree (more than one optimal code assignment). Provided thats
it's random if '0' or '1' is assigned to 'A' - then a brute-force
attempt faces many possibilities (more in my article, I'll post an
updated version with error corrections - if you - or any - will care to
read it...
Remember, compression will perform badly on some (random) data. It may
end up only permutating the symbols (classic simple encryptation
problem). Thus it may not provide the same security level all the time.
But if the brute-force attacker don't know that it has just been a
permutation, he can't (won't) look for that first - and by the way, he don't
know the symbols - they're encrypted in the model (still guessing isn't
always hard). 

>the trees or models. Whether such people are on the group or not,
>I would not trust these compression methods for secrecy without 
>establishing that they defeat the methods used in the old days to
>crack well-constructed codebooks. Such codebooks gave good compression
>by modern standards, using the approximate Huffman idea of providing
>single codewords for common long strings and for uncommon short strings.
>This precaution made their codes much stronger, but not unbreakable.
>The analysts often discovered that a particular military situation
>made some strings yet more common than was allowed for. The same
>thing can presumably be done to Huffman codes and arithmetic codes.
>Adaptive coding is not the answer, because the analyst may know
>context or current events, so he adapts instantly, while the code
>needs a large sample to adapt.
>-- 
>Stewart C. Strait
>straits@netcom.com

How can some strings in huffman be more common than others???
If such a 'more common' string pointed to a symbol, why wasn't it moved
down in the tree to give a shorter code --- doesn't sound like optimal
coding to me... Well, I popbably got you wrong... 

All the best,
Stig Valentini

