Path: msuinfo!agate!howland.reston.ans.net!math.ohio-state.edu!jussieu.fr!univ-lyon1.fr!swidir.switch.ch!scsing.switch.ch!elna.ethz.ch!caronni
From: caronni@nessie.cs.id.ethz.ch (Germano Caronni)
Newsgroups: sci.crypt
Subject: Re: "Lossy" encryption?
Date: 7 Apr 1994 08:47:45 GMT
Organization: Swiss Federal Institute of Technology (ETH), Zurich, CH
Lines: 17
Distribution: world
Message-ID: <2o0hbh$dsl@elna.ethz.ch>
References: <2nuhj6$kh2@yeshua.marcam.com>
NNTP-Posting-Host: nessie.ethz.ch

In article <2nuhj6$kh2@yeshua.marcam.com> davidm@marcam.com (David MacMahon) writes:
>I was thinking about using FFTs as part of an encryption scheme, but 
>I realized that the decrypted (via inverse FFTs) data would not be 
>exactly the same as the original due to rounding errors.  This would 
>provide a lossy encryption, analgous to lossy compression.  If error  
>detection and correction bits were added to the original data, the 
>lossy encryption could be rendered "lossless".
>
Hmm, an intriguing idea! But wouldn't the added 'detection and correction bits'
cause additional redundancy which would reduce the strongness of the cipher ?

Perhaps you could 'prepare' the data as to go thorugh the FFT without rounding
errors, but then, what would you use as 'key' to the encryption algorithm?

Friendly greetings,

	+gec
