Path: msuinfo!agate!dog.ee.lbl.gov!ihnp4.ucsd.edu!swrinde!news.uh.edu!ccsvax.sfasu.edu!hoz.telescan.com!RICK
Newsgroups: sci.crypt
Subject: Re: Decompressing ciphertext
Message-ID: <RICK.4.000C69C5@telescan.com>
From: RICK@telescan.com (Rick F. Hoselton)
Date: Mon, 25 Apr 1994 12:24:42
References: <RICK.1.00121813@telescan.com> <16F9FF70CS86.C445585@mizzou1.missouri.edu> <2p8iu6$bp7@ccu2.auckland.ac.nz>
Organization: Telescan
Keywords: compression, Huffman, Arithmetic coding
Nntp-Posting-Host: 198.67.14.194
X-Newsreader: Trumpet for Windows [Version 1.0 Rev A]Lines: 14
Lines: 14



>This has already been done - in Bell, Witten, and Cleary's "Text 
>Compression" the authors build up a model of the Brown Corpus (a very large
>body of English text), and then use it to "decompress" random noise, as an
>experiment in seeing how well the compressors can approximate the original
>source statistics (the result: Not very well).
> 

Thanks, I wondered where I got the idea.  But the authors state that the 
"low quality of the output corresponds to the relatively poor compression
achieved...."  I was using a higher order model than theirs, and my text 
looked MUCH better than the example they give on page 255.  If anybody
is really interested, maybe I can find some of the text generated and post it.
