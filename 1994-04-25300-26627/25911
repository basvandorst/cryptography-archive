Path: msuinfo!harbinger.cc.monash.edu.au!yeshua.marcam.com!MathWorks.Com!europa.eng.gtefsd.com!news.umbc.edu!olson
From: olson@umbc.edu (Bryan G. Olson; CMSC (G))
Newsgroups: sci.crypt
Subject: Re: Recognizing Plaintext...
Date: 12 Apr 1994 20:20:27 GMT
Organization: University of Maryland, Baltimore County
Lines: 48
Message-ID: <2oevqb$mrc@news.umbc.edu>
References: <slansky_doug-070494084511@mac-an-48.cig.mot.com> <766014357Stu.stu@nemesis.wimsey.com>
NNTP-Posting-Host: umbc7.umbc.edu
X-Newsreader: TIN [version 1.2 PL2]

Stuart Smith (Stu@nemesis.wimsey.com) wrote:
: Any of the large scale DES-cracking machines that have been proposed
: *require* a plaintext ciphertext pair - that is 64 bits of ciphertext
: along with the original plaintext.  If you can't get this then you can
: still do brute force, but then, as you suggest, you need to check
: character frequencies and such, making a DES breaking machine
: considerably more complex.  

A little knowledge about the plaintext goes a long way.
Suppose you know the plaintext bytes will all have 0 in the
high bit, as ASCII text will.  Checking this could easily be
done in hardware, at an added cost of a few percent.

Now we have the problem of false hits.  We expect decryption
with an incorrect key to give us an essentially random block,
so one one in 256 keys will give us a false hit.  Given a
false hit, we then try the key on the next block, which also
has about a 1/256 chance of a false hit.

In general if x is the chance of a false hit, the expected
number of trial decryptions to reject an incorrect key is,

   __infinity
   \    
   /__    x^i      = 1/(1-x)     for x < 1. 
     i=0

For x=1/256 this is about 1.004.  I don't think a brute-force
DES cracker for ASCII text is much more expensive to build or
run than a known-plaintext brute-force cracker.  I would guess
that the reason the proposed crackers assume known plaintext
is that the point of the papers is the weakness of DES, not
the benefits of actually building such a machine.

Given a more general model of language, we would have to worry
about false negatives as well as false positives.  The hardware
would have to be more complex, and we may require more trial
decryptions by a reasonable constant factor.

I'm not sure if it has appeared yet, but a paper by Ravi
Ganesan and Alan Sherman, "Statistical Techniques for Language
Recognition" has been accepted for publication by
_Cryptologia_, and it describes a Markov model of language and
how to set up statistical tests with the required power and
strength.


--Bryan Olson
