Newsgroups: sci.crypt
Path: msuinfo!uwm.edu!vixen.cso.uiuc.edu!howland.reston.ans.net!EU.net!dkuug!uts!news.uni-c.dk!id.dth.dk!id.dth.dk!sv
From: sv@idfs3.uucp (Stig Valentini)
Subject: Faster crypt of compressed data (revised)
Message-ID: <sv.766739639@id.dth.dk>
Date: Tue, 19 Apr 1994 07:13:59 GMT
Lines: 274


---------------------------------------------
Faster cryptation of already compressed data
(by Stig Valentini)  Revised version.
---------------------------------------------

Idea
----
  If encryption was to be applied on already compressed data, then it may
not result in any (significant) gain of security - assuming that the compression
algorithm was sufficiently effective (compressed into a near to random
distribution). Therefore it may not be necessary to encrypt all data coming from
a compressor (only those having a non-random distribution - for example the
'model' used for subsequent uncompression).

Initially
---------
  Only open-systems are considered. This goes for both compression and
encryption processes. 

Compression/cryptation similarities and differences
---------------------------------------------------
  Classic model:
'input'-->'compressor'-->'encryption'-->'error correcting'-->'transmitting'
(and the reverse order to restore the original data).
  Still the common goal of compression and cryptation is randomness.
It's obviously a problem if a compression process can't remove all redundance
from input data - because this will result in less efficient packing of the
data. Only a random distribution can't be compressed further. Also if the
distribution of encrypted data doen't turn out random, then it should in
theory be possible to extract information from the data thus possibly breaking
the encryptation.
  Entropy does in theory set a limit for how random a compression or
encryption process can make a distribution. It must not become random to the
extent that actual information disapears - then the process would obviously
not be reversible.
  Still, the two processes basicly differ regarding size and time restrictions.
Encrypted data is allowed to exceed the original data in size - and time-
consuming algorithms is prefered in order to prevent encryption brute-force
attacks. On the other hand, compressed is expected to result in a size
reduction - and fast algorithms are favored (if the compression time exceeds
the time saved when transfering the reduced amount of data over a channel -
then it would be easier just to transmit the uncompressed data).
  Basicly compression comes in two different forms: static and dynamic.
  Static compression first creates a fixed model of the (finite) data which
is subsequently used for the encoding. In order to successfully decode the
data, then it's necessary to submit the model with the encoded data during a
transmission. Static compressed data thus results in two separate parts:
(a) the 'model' (for example a tree in case of Huffman compression), (b) the
encoded data formed during the compression process. If the compression
algorithm is sufficiently effective, then (b) should be a near to random
distribution. In that case during en encryption process, then it's only
necessary to encrypt the model (a finite, well-defined set of data) and not
the encoded data. Because mapping an already random distribution to another
random distribution doesn't serve any purpose (information theory dictates
than ordered information cannot be extracted from a truly random distribution).
  Dynamic compression integrates the 'model' into the encoded data - and
presents different problems. In order to keep this text simple - and focus on
the main idea - then only static compression is considered.


Potential problems
------------------
  While encryption is a non-deterministic process (the password introduces the
non-deterministic element) - then the compresion process is usually a
deterministic process. But the latter is not a consequence of theory - but
rather due to implementation efficiency (coding and subsequent execution). In
the simple case of static Huffman-encoding 'AABC' there's four possible bitcode
assignments ('A','B','C')=(0,10,01), (0,01,10), (1,00,01) or (1,01,00). All
assignments will result in same size reduction - but give different output
bitpatterns. Confronted with the same problem then a deterministic process will
always chose one and the same bitcode assignment. This imposes a problem seen
from a security perspective. If a brute-force decompression attack was to be
launched on the previous output data with 'AABC' as a guess (not knowing that
it was the actual solution) - then only 1 test would be necessary in the case
of a deterministic compressor (being an open system process it's known which
of the 4 code assignments it would prefere) - while testing for all 4
possibilities would be necessary for a non-deterministic compressor.
  For the simple Huffman example above, then a non-deterministic compression
process isn't less efficient (it still choses 1 of the 4 optimal 'models') and
doesn't take more time (the selection of coding models could be based on
the more or less random position of the computer clock at the time of the
process - a quick check process - and once the model is selected then the
actual encoding process is the same for each of the 4 optimal trees).
  Why aren't more compression algorithms non-deterministic if the performance
is approximately the same? Well, the sourcecoding is much more complex with the
introduction of non-deterministic 'model' selection. If the sourcecoding isn't
done efficiently, then the complexity could quickly result in time-performance
degrading. And in the Huffman case (as for other algorithms) there may be a
slight data overhead when the tree itself is encoded and saved with the
compressed data. Whith more possibilities for the tree, it's harder (takes
more space) to describe. This could result in an overall space-performance
degrading.
  If a subsequently encryption was not to be performed, then the possible
time and space degrations wouldn't be tolerated. But if a subsequently
encryptation was to take place - then it need only be applied on the 'model'
(which is usually only a slight fraction of the totally amount of compressed
data). The need of encrypting less data would result in a great time-gain.
  Looking more generally at the process, then the non-deterministic compressor
to some extent replaces password-encrypting.
  Different from what what may have been expected - this may in fact result
in improved security!!!
  When a person chooses a password - at whatever level in the encryption
algorithmn - then it's usually not a truly random process. If that was the
case, then there would exist no 'hackers', because even brute-force methods
give up on truly random distributions. But by introducing random elements
into a non-deterministic compression process - you eliminate the need of 'human'
interference and can rely on truly random processes. Sure, you still need to
apply a 'human' made password for the subsequent encryption of the compression
'model' as described above - but this will at least not give less security than
applying the password on all the data. - A chain isn't stronger that the
weakest link... But now you may send the human-encrypted part by a (high-cost)
high-security channel - and just send the other part by a normal channel.
  But remember, that if the compression algorithm performs badly (producing
non-uniform distribution) then it will also give low security.

  Going back to a deterministic compressor process. A brute-force attack
could be concentrated on the compressed data - ignoring the encrypted 'model'.
But then the compression could not be considered strictly deterministic.
By loosing the 'model', it's actually a lossy process. Its level of security
now depends on how much information is lost.
  Consider the worst case: if the input data already is a random distribution,
then the compression process is reduced to a simple permutation process of the
symbols in the input data. For an N-symbol alphabet this gives N! (faculty)
possibilities. That's an astronomic number. Only by prior knowledge of portions
of original data is it possible to perform a full or partially re-permutation.
So in the worst case a compression doesn't perform worse than a classic
encryptation process.
  In other cases where symbols may occur more or less frequently, then there
will be an increase in the number of possible code assignments - increasing
the work for a brute-force attack.

Security in the case of Huffman compression
-------------------------------------------
  The most widely used compression method - Huffman - is known only to be
optimal in the case where the symbols in the data has probabilities in the
order of 2^N. All other cases will present problems in form of non-randomness
in the compressed data (2). Arithmic compression should on the other hand
always provide uniform randomness - and thus presents no problems.
  Decoding Huffman-encoded data without knowledge of the symbol distribution
isn't easy though. Suppose you knew the number N of used symbols. First
hurdle would be to to figure out the number M of possible tree distributions:

N=2 gives M=1 possible tree.
N=3 gives M=2 possible tree configurations (0=left,1=right):

            codes 0        | |       codes 1     | |
                  10    |  ---             00    ---  |
                  11    ----|                     |---|
                          |                         |
N=4:
        | |      | |          | |       | |
     |  ---      ---  |    |  ---       ---  |
 |   ----|    |   |----    ----|   |     |----   |
 ------|      ------|        |-----|       |------
    |            |              |             |

    0            0              1             1       2^(N-2)
    10           11             00            01      configurations
    110          100            010           000     with max codelenght
    111          101            011           001

   | | | |
   --- ---
    |---|
      |

      00         2^(N-2-2)*1
      01         configurations
      10         with two codes
      11         starting with 0

thus M=5

N=5:

0     0     0     0     1     1     1     1        2^(N-2)
10    10    11    11    00    00    01    01       configurations
110   111   100   101   010   011   000   001      with max codelenght
1110  1100  1010  1000  0110  0100  0010  0000
1111  1101  1011  1001  0111  0101  0011  0001

00    00    10    10                               2^(N-3)
01    01    11    11                               configurations
10    11    00    01                               with two codes
110   100   010   000                              starting with 0
111   101   011   001


Please notice that 4 codes starting with 0 would bring us back to the first
case. Thus M=2^(N-2)+2^(N-3)=8+4=12

N=6: M = 2^(N-2)+2^(N-3)+2^(N-4) = 16+8+4 = 28

Arbitrary N:
              (1)       (2)                     [N/2]
N odd:   M = 2^(N-2) + 2^(N-3) +...+ 2^(N-3) + 2^(N-3) = ([N/2]+1)  *2^(N-3)
N equal: M = 2^(N-2) + 2^(N-3) +...+ 2^(N-3) + 2^(N-4)
           = 2^(N-3) * ([N/2]-2+2+1/2)                 = (2*[N/2]+1)*2^(N-4)
----------------------------------------------------------------------------
General: M = (N+1) * 2^(N-4)         (for N>2)

  And in each tree it's possible to re-arrange the symbols in N! (faculty)
ways (N places to put the first symbol, N-1 to put the next,...).
This gives totally N!*(N+1)*2^(N-4) = (N+1)!*2^(N-4) possibly trees.
  In most cases only the interval in which N lies will be known - not(!) the
exact value of N. So most probably it will be necessary to try several
different values of N. Imagine the common case of N=256 (Huffman-tree of
extended ASCII-set). And after you've tried 257!*2^252 trees,
it's on to N=255 ...
  This has been a discussion of the worst possible cases. In most Huffman-
implementations all Huffman-tree configurations will not be possible.
For example will the most frequent symbol 'A' in the string 'AABC' in some
cases always be assigned the code '0' (never '1'). Of several possibilities
where the set of bitcodes has identical length (and all are optimal) - such
as ('A','B','C') = (0,10,01),(0,01,10), (1,00,01) or (1,01,00) - then a
deterministic Huffman coder may always choose one and the same. Now consider
only the cases, where Huffman-trees resulting in different set of bitlenghts.
If N=3 like the former case 'AABC', then M=1 (all four possibilities have
bitlenghts 1,2,2). If N=4, then M=2 (either bitlenghts 2,2,2,2 or 1,2,3,3). In
the general case it gives M=[N/2]. This is a dramatic reduction from
(N+1)*2^(N-4). - The lesson learned from this, is that a deterministic Huffman
coder (with [N/2] possible trees) is much, much less safe than a non-
deterministic Huffman coder (with (N+1)*2^(N-4) trees) - seen from a encryption
attackers point of view. Still, this don't change the permutation factor of N!.
  By using heuristic knowledge that 'e' may be more common than 'z' in a text
- and that 'e' thus may be on a lower level than 'z' in the tree - then it's
possible to reduce the factor N!. But N! is a big number - so you'll
need lots of heuristic knowledge to get down on a plane where you may handle
the problem. If you only need to decompress (decrypt) the data partly,
then you can naturally do with less heuristic knowledge.

  Suppose a non-deterministic compression coder is used. Even if you have
heuristic knowledge of the original statistical distribution of the symbols,
then you'll only know the bitlenghts of the codes used for the symbols. If
the codes have identical codelenghts, we're back at the simple permutation
of symbols in the original data - fairly easy to hack - but if the codes have
differently bitlengts, then you're facing around 2^(N-3) differnt sets of
code assignments. Finding out where they begin and end in the data stream will
not be impossible - but each time you miss a bit, you're lost in garbage...


Conclusion
----------
  In the case of the Huffman tree, then - depending on a non-deterministic
design of the algorithm - it can actually be made quite difficult to decode
Huffman-encoded data without information contained in the de-coding tree. And
it can probably be made close to impossible to decode arithmic compressed data
without the original 'model'. This requires a minimum of computation overhead
- compared to what's needed for a good encryption. So for most purposes it's
sufficient only to compress the compression 'model' and not the actual
compression-encoded data.
  This will result in saved time (less actual data to encrypt) compared to most
normal cases where all the data (either original or compressed) would have
been encrypted - and still aproximately maintain the same level of security.
And if the saved time was invested in a more complex encryption algorithm,
then it would actually result in better security than before (while not using
more time).
  In cases where a deterministic compressor coder is used, then it's still
possible to 'repair' the degraded security: get a truly random password,
encrypt the compressed data with this password (not the 'model') using a fast
(simple) encryption algorithm, append the password to the 'model' and encrypt
the latter with your encryption algorithm using your private password. - Using
a random number generator, it shouldn't be too difficult finding a proper
password for the fast encryption. The process can be done hidden from the user
of the encryption (compression) program. Assuming that the random generator
gets its input feed from the computer clock (timer) secunds ticks (fairly
random when you start the process) - then you should be rather sure of a
random password - better that what a human may figure out (it's hard for a
human to figure out - and remember! - a long, random password). Using
different passwords for the compressed data and the 'model' is important in
order to prevent exposing the user defined password by the fast (simple)
encryption to hackers.

