Newsgroups: sci.crypt
Path: msuinfo!agate!darkstar.UCSC.EDU!news.hal.COM!olivea!charnel!charnel.net.csuchico.edu!nic-nac.CSU.net!usc!cs.utexas.edu!howland.reston.ans.net!EU.net!dkuug!uts!news.uni-c.dk!id.dth.dk!id.dth.dk!sv
From: sv@idfs3.uucp (Stig Valentini)
Subject: Re: Quick crypt of compressed data
Message-ID: <sv.766669321@id.dth.dk>
References: <sv.766048428@id.dth.dk> <Co4I5v.Lvq@ulysses.homer.att.com>
Date: Mon, 18 Apr 1994 11:42:01 GMT
Lines: 37

smb@research.att.com (Steven Bellovin) writes:

>In article <sv.766048428@id.dth.dk>, sv@idfs3.uucp (Stig Valentini) writes:
>> If I've got it right, then normal archivers (ZIP,ARJ,..) encrypt
>> all data in a file - but why not just encrypt the huffman tree data
>> preceeding the huffman encoded data. Without the huffman-tree info,
>> the remaining data should be fairly random, and therefore hard to
>> brute-force decrypt. Using this idea would make encryption/decryption
>> much(!) faster with the same level of security offered.

>I doubt that that would work.  Someone with a decent idea of the
>statistics of the plaintext could make decent guesses about the

BTW by knowing that you're dealing with a text - you're in fact assuming
heuristic knowledge. Heuristic knowledge also hurts most other
encryption systems. What you're in fact stating is that such heuristic 
knowledge in the case of compression hurts the system more that other encryption
systems. Well, I'll have to think about that...

>compression dictionary, and recover a lot that way.  The rest
>could be filled in piecemeal.  If nothing else, what you suggest would
>boil down, in essence, to a code book -- the cryptanalyst would ignore
>the dictionary and just worry about the code.

Anyway, I see you point. From one of the teachers (professors) I've also
heard the argument that it would be possible to decompress the encoded
data partially (and in some cases that's all what's necessary). But
where would you know to start and stop in the (fairly random) bitstream.
Provided you use an original Huffman which just give you an optimal tree
(non-deterministic compressor) then decompression involves testing a lot
of permutations  -  and if you get the bitcode length of a symbol wrong at
any point, you're lost in garbage...
BTW I only mention Huffman, because it's so easy to discuss (and the one
I've grown most used to) - not because I think it's the top of the
world.
 

