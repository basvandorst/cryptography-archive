Path: msuinfo!agate!howland.reston.ans.net!wupost!udel!pacs.sunbelt.net!lynx.unm.edu!dns1.NMSU.Edu!opus!ted
Newsgroups: comp.ai,comp.ai.nat-lang,comp.compression,sci.crypt
Subject: Re: Proposed index definition for standardized dictionaries
Message-ID: <TED.94Apr3090132@lole.crl.nmsu.edu>
From: ted@crl.nmsu.edu (Ted Dunning)
Date: 3 Apr 94 09:01:32
References: <2ncet5$a2q@ccu2.auckland.ac.nz>
Organization: Computing Research Lab
NNTP-Posting-Host: lole.nmsu.edu
In-reply-to: sig@Seuss.Vantage.GTE.COM's message of 30 Mar 1994 18:03:17 GMT
Lines: 73
Xref: msuinfo comp.ai:21595 comp.ai.nat-lang:1488 comp.compression:11838 sci.crypt:25455



In article <2ncet5$a2q@ccu2.auckland.ac.nz> sig@Seuss.Vantage.GTE.COM (Sigurd Crossland) writes:

   As an aid to those involved in natural language parsing, dictionary
   compression, foreign language translation, or textual encryption, I
   have been collecting and compiling a lengthy list of words.  It is
   expected that a comprehensive standardized dictionary will eventually
   result.  This dictionary should contain most common American words,
   abbreviations, acronyms, hyphenations, and even incorrect spellings.

   This draft will document the proposed index structure used to access
   the standardized dictionary.  Comments and criticisms are welcome.

   'Words' are to be sorted by length, stored in ascending ASCII
   collating sequence and normalized to lower case where possible to save
   space.  Words containing unusual capitalization - other than all
   lower, all upper, or first letter raised - will be represented as a
   separate unique entry in the dictionary.

   It is anticipated that a comprehensive dictionary can be compiled from
   words addressed within a space of 1 MB (2^^20 or 1,048,576).

quite frankly, there are *very* simple techniques which will give you
much better than the 8:1 compression you seem to be after.

in particular, since this is just a word list the standard hack of
prefix encoding should be examined.  in this method, the standard
character set is augmented by several special characters which
indicate that 0 or more characters of the current word are the same as
the previous word.  in addition we need a word termination character.

for sig's word list (not a dictionary, really), prefixes up to about
30 characters may be needed.  since this is also an ascii dictionary,
we could go ahead and pick 100 or so common substrings to encode in
the upper half of the code table (limiting the utility of the scheme
for european languages).

now we can just use a fairly conventional compression program to mash
the result into a smaller space.  eric iverson has experimented with
these techniques quite a bit and i believe he has gotten quite a bit
better results than the 8:1 compression sig is aiming for.

of course, simple minded compression of the entire prefix coded word
list is not all that good for many applications.  sometimes we would
like to find a word at random rather than just list all the words in
the list.  to do this, we just do the final compression on reasonably
short blocks of prefix coded words and remember what word is at the
beginning of each block.  this means that searching consists of
looking through the list of block pointer to find the correct block
and then serial decompressing the block until the word of interest is
found.  caching of common words could radically decrease the cost of
this searching method.

these simple methods have the following advantages:

1) they are simple to implement and explain

2) they work very well at the primary goal which is to compress the
dictionary.

3) they do not seriously inhibit searching the dictionary

4) they do not put limits on the upper lower case content of the word
list.

5) they do not impose the inherent limits of fixed size data
structures and fixed format records.

given these advantages, i don't see any point in the scheme that sig
is proposing.


