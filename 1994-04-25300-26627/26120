Newsgroups: sci.crypt
Path: msuinfo!agate!darkstar.UCSC.EDU!news.hal.COM!decwrl!netcomsv!netcom.com!straits
From: straits@netcom.com (Stewart C. Strait)
Subject: Re: Quick crypt of compressed data
Message-ID: <straitsCoFrJD.867@netcom.com>
Organization: NETCOM On-line Communication Services (408 241-9760 guest)
X-Newsreader: TIN [version 1.2 PL1]
References: <sv.766048428@id.dth.dk> <sv.766220553@id.dth.dk> <sv.766309898@id.dth.dk> <straitsCoCEpp.J74@netcom.com> <2oo9fr$anl@agate.berkeley.edu>
Date: Mon, 18 Apr 1994 03:40:25 GMT
Lines: 39

Nicholas C. Weaver (nweaver@boojum.CS.Berkeley.EDU) wrote [in part]:

: In article <straitsCoCEpp.J74@netcom.com>,
: Stewart C. Strait <straits@netcom.com> wrote:
: >The vital point that seems to be overlooked here is that _all_ compression
: >algorithms used on large volumes of text are _highly_ nonoptimal.

: 	From what I've learned in Information Theory so far, this is not the
: case.  Universal Source Coding (such as Lempel-Ziv ) only become optimal
: with large volumes of text.  These codes do not depend on the probability
: distribution of the input, only that the distribution doesn't change (is
: ergotic).  Thus, to compress _The Complete Works of William Shakespeare_,
: L-Z will work very well, but to compress "Hello World", Lempel-Ziv will
: inflate things consiterably.  Unix "compress" uses Lempel-Ziv.  
I can't say that I can offer a real proof , but I always thought that
the rate of convergence to optimality of adaptive compressors was
a function of the probability structure of the input, and that text
in natural languages converged slowly enough that even all of Shakespeare
would compress to something with measurable redundancy left.

Also, I think that unchanging probability structure ('stationary' 
rather than 'ergodic' I think) would imply that subject matter and
context were unchanging or all-inclusive. In a military history
example this would mean a sample so long that history repeated itself
at random. BTW, the blunt example is not meant to imply that you
overlooked anything obvious--I think this stuff is not at all
obvious.

On a related subject, an email correspondent suggests something
about arithmetic coding depending on all previous bits of the input.
I'm not sure what he means, but I think the probability of actual
dependence falls off with the separation so that the cryptographic
strength of these codes, when the probability model is poor, is
also poor. It would be nice if someone has actually broken this
stuff and would care to comment, but using computer-era crypto
for a puzzle is a bit rare.
-- 
Stewart C. Strait
straits@netcom.com
