Path: msuinfo!agate!howland.reston.ans.net!EU.net!Germany.EU.net!Munich.Germany.EU.net!ibm.de!aixssc.uk.ibm.com!watnews.watson.ibm.com!locutus.rchland.ibm.com!wo0z.rchland.ibm.com!lwloen
From: lwloen@wo0z.rchland.ibm.com (Larry Loen)
Newsgroups: sci.crypt
Subject: Re: Encryption & Compression
Date: 19 Apr 1994 18:35:06 GMT
Organization: IBM Rochester MN
Lines: 75
Distribution: world
Message-ID: <2p188q$1e8m@locutus.rchland.ibm.com>
References: <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>
Reply-To: lwloen@rchland.vnet.ibm.com
NNTP-Posting-Host: wo0z.rchland.ibm.com

In article <CHAR-LEZ-190494085216@dnwmac194.globalvillag.com>, CHAR-LEZ (Char-Lez Braden) writes:
|> Help me understand something about these technologies, they seem related.
|> 
|> As I understand it, a perfectly destributed* random set of data held in a
|> file, is compressable.
|> 
|> ALSO, the goal of an encryption algorithym is to take a set of data that
|> has some organization to it, and obscure that organization beyond all
|> recgnition to those who do not have authorization.
|> 
|> Are either of these points wrong?
|> 
|> Why then, if I take a file, compress it, then encrypt it and compress it
|> again, it does not compress more?  This says to me that the compression
|> routine is still finding some inate order to the set of data.
|> 
|> I've gone to no small trouble to demonstraight that this is the case.
|> 
|> I've even tried One-Time Pad encryption routines, with the same results. 
|> That bothers me too, does this mean that data+Chaos<>Chaos?  Once in order,
|> always in order?!?
|> 
|> Am I missing something fundamental?
|> 
|> One of my Comp instructors brought up the "law of entropy" (Of which I know
|> nothing) and said that a bit can only hold so much data.  I can accept
|> that, but I cannot explain it.  My brain hurts.
|> 
|> *No factor shows up more than any other.
|> 
|> Thanks,
|> Char-Lez


Think of it this way; compression works because some things happen more
often than other things.  For instance, in this posting, the letter "e"
will happen much more often than the letter "z".

There are a great many compression algorithms, but they are all based
on using fewer bits for the "e"s and, because there are still only 256
possible byte values, coming up with a scheme that uses at least 9 bits
for the "z"s.  As long as "z"s are fewer than "e"s, you win overall.

With text strings, there is opportunity to "win" with compression.  But,
for some other kinds of data, such as a "PCX" picture file of woodlands,
there may be little or no opportunity to "win" with compression, because
the equivalent of "e" and "z" are too close together in probability, due
to about even distribution of (say) all 256 colors in the picture in random
sequence.

This all applies not only to individual characters, but groups of characters,
too.  So, if we just talk about trying to represent individual characters
in fewer than 8 bits for the "popular" input bytes and more than 8 bits for 
the rarer ones, it will be good enough for this discussion.  So much for
compression.

A good encryption algorithm, whatever else it does, has some sort of
relationship that takes input, transforms it, and produces output.  The
output, to be any good cryptographically, must satisfy many properties.
One bare minimum property is that it must do a reasonably good job
making the "e"s and the "z"s come out in about the same amounts.  Therefore,
it is hard to compress an encrypted text, because the tricks that make
compression work have disappeared as the result of decent encryption.
In effect, encrypted text is similar in properties to the "PCX" picture file
of woodlands.

Therefore, if you want to use compression, compress before encrypting,
not after, because afterward it is a lousy input source for compression.


-- 
   Larry W. Loen        |  My Opinions are decidedly my own, so please
                        |  do not attribute them to my employer

   email to:  lwloen@rchland.vnet.ibm.com
