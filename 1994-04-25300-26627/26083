Newsgroups: sci.crypt
Path: msuinfo!uwm.edu!cs.utexas.edu!usc!howland.reston.ans.net!agate!library.ucla.edu!csulb.edu!csus.edu!netcom.com!straits
From: straits@netcom.com (Stewart C. Strait)
Subject: Re: Quick crypt of compressed data
Message-ID: <straitsCoCEpp.J74@netcom.com>
Organization: NETCOM On-line Communication Services (408 241-9760 guest)
X-Newsreader: TIN [version 1.2 PL1]
References: <sv.766048428@id.dth.dk> <Co4I5v.Lvq@ulysses.homer.att.com> <sv.766220553@id.dth.dk> <sv.766309898@id.dth.dk>
Date: Sat, 16 Apr 1994 08:10:36 GMT
Lines: 41

Stig Valentini (sv@idfs3.uucp) wrote [in part]:
: If the distribution of compressed data isn't close to random, then the
: compression algorithm cannot be considered optimal - because by removing the
: remaining redundance it should be possible to compress the data
: further.

The vital point that seems to be overlooked here is that _all_ compression
algorithms used on large volumes of text are _highly_ nonoptimal.
Low-order statistics like single-byte frequencies are easy to predict
statically in some applications, and usually easy to adapt to, but
things like subject matter, favorite vocabulary, style, etc. cause long
strings to be repeated or repeated with slight modifications.
No one has enough memory to store a good enough probability model,
nor enough data to get the model close enough, either adaptively or
in advance. 

:   Time to get down to a conclusion:
:   In the case of the Huffman tree, then it's actually quite difficult to
: decode Huffman-encoded data without information contained in the de-coding tree.
: And it's probably close to impossible to decode arithmic compressed data
: without the original 'model'. So for most purposes it's sufficient only to
: compress the compression 'model' and not the actual compression-encoded data.

IMO, this is dead wrong. I suspect that there are several people
on this newsgroup who have broken Huffman or arithmetic codes without
the trees or models. Whether such people are on the group or not,
I would not trust these compression methods for secrecy without 
establishing that they defeat the methods used in the old days to
crack well-constructed codebooks. Such codebooks gave good compression
by modern standards, using the approximate Huffman idea of providing
single codewords for common long strings and for uncommon short strings.
This precaution made their codes much stronger, but not unbreakable.
The analysts often discovered that a particular military situation
made some strings yet more common than was allowed for. The same
thing can presumably be done to Huffman codes and arithmetic codes.
Adaptive coding is not the answer, because the analyst may know
context or current events, so he adapts instantly, while the code
needs a large sample to adapt.
-- 
Stewart C. Strait
straits@netcom.com
