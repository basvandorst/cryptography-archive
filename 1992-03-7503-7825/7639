Newsgroups: sci.crypt
Path: msuinfo!caen!zaphod.mps.ohio-state.edu!mips!news.cs.indiana.edu!att!walter!qualcom.qualcomm.com!qualcom.qualcomm.com!karn
From: karn@qualcom.qualcomm.com (Phil Karn)
Subject: Re: MD[45]
Message-ID: <1992Mar11.231020.1188@qualcomm.com>
Sender: news@qualcomm.com
Nntp-Posting-Host: qualcom.qualcomm.com
Reply-To: karn@chicago.qualcomm.com
Organization: Qualcomm, Inc
References: <i4fhbhs@sgi.sgi.com> <1992Mar11.152032.2317@unx.sas.com>
Date: Wed, 11 Mar 1992 23:10:20 GMT
Lines: 96

In article <1992Mar11.152032.2317@unx.sas.com>, sasrdt@shewhart.unx.sas.com (Randall D. Tobias) writes:
|> Huh?  How can I judge the truth of their conjecture about its [MD-4/5]
|> irreversibility?

A good question. I have not seen anything in print about the design
philosphy of either MD-4 or MD-5 other than the authors' assertions
about the difficulty of inverting it or of finding two inputs that
hash to the same output. But I have examined the algorithm in detail
and can make some guesses about why it works, with the caveat that I'm
an implementer of crypto algorithms, NOT a cryptographer. So this is
all purely speculation on my part. Anybody who knows better is welcome
to comment.

The basic idea seems to be to make a function which is as chaotic as
possible. That is, small changes in the input (e.g., 1 bit) are
designed to affect all of the output bits in complex ways as quickly
as possible. (They call this the "avalanche effect" in the MD-5
Internet Draft.) If you tried to solve for the input variables as a
function of the output variables by working the rounds backwards, the
boolean complexity would quickly mushroom. This makes an analytic
attack infeasible.

To prevent statistical attacks, all of the mixing functions in MD-5
are balanced, that is, if the inputs are statistically independent and
unbiased, so are the outputs. (This is explicitly mentioned in the
MD-5 Internet Draft).  The many rounds combine relatively simple (and
therefore fast) substitution and permutation operations. This has been
a proven technique for building strong ciphers ever since Shannon's
original paper.

|> The sqrt(2) and sqrt(3) which show up look particularly
|> suspect: why not pi or e or (10! mod 2^16)?

Sqrt(2) and sqrt(3) are both irrational numbers. The intent was
probably to produce a statistically "random" pattern of bits in a
clearly described way. True random numbers probably would have been
better, but then you'd be asked to "prove" to everyone that they're
really and truly "random", without any trap doors.

MD-5 uses different additive constants for each step, 64 in all.  They
are generated by the formula 4294967296*abs(sin(i)) where i is the
step number. The sine function produces transcendental values for
non-zero integer radian arguments, and 4294967296 is 2^32, so this
formula also appears to produce seemingly "random" 32-bit numbers, at
least in the low order bits. Fortunately there's plenty of bit
rotating to spread out the effects of the substitution operations
across the bits of each word.

|> I wasn't clear enough.  Part of the "overhead" which we don't want is
|> making the old versions of files available for comparison in the first
|> place.  Shipping hashed digests of old versions is to be a substitute
|> for shipping megabytes and megabytes of old versions themselves.

I think this is a very good application for a hash function like MD-5.
Detecting files modified by viruses or trojan horses is one obvious
use.

I would like to see FTP servers provide a command to compute and
return the MD-5 hash for a specified file so you could build automatic
FTP clients that would compare files across machines on a slow network
without having to actually ship the files around. The "archie" service
could really benefit from this since you can't always tell from name
and file size whether a pair of files really are the same. It would
*especially* help in finding identical files with different names (and
dates) on different machines.

A few weeks ago I threw together a program to find duplicate files in
a MS-DOS or UNIX file system. My approach was to first sort the files
by size. Then for each file with a non-unique size, I computed their
MD-5 hash values and sorted each set to find duplicates.

It worked, but not as fast as simply sorting the list of files using a
file comparison function, passed to qsort(), that first compared the
file sizes. If the sizes were equal, it went on to compare the files'
contents, block by block, until either a difference was found or EOF
was encountered.

Of course, performance depended heavily on the actual distribution of
duplicate files, along with the relative speeds of CPU and disk. My
idea behind using hash functions was to require that each file be read
only once at most; all further comparisons (e.g., requested by qsort)
would operate only on the hash codes in memory.  But I discovered that
at least for my MS-DOS system at home, the non-hash-function
comparison program ran twice as fast largely because the comparison
function could return as soon as it detected a difference between the
files; it didn't have to read the whole thing, even though each file
was opened and closed many more times. It also helped to have a big
RAM cache; I suspect the hash-based version would have performed
relatively better on a machine without disk buffering.

By the way, this was on a 33 Mhz 486 with a 330 megabyte 18 ms SCSI
disk drive using my 386 assembler implementation of the MD-5 transform
primitive.

Phil

