Path: msuinfo!caen!spool.mu.edu!agate!stanford.edu!rock!sas!mozart.unx.sas.com!wagner.unx.sas.com!sasrdt
From: sasrdt@shewhart.unx.sas.com (Randall D. Tobias)
Newsgroups: sci.crypt
Subject: Re: MD[45]
Message-ID: <1992Mar11.152032.2317@unx.sas.com>
Date: 11 Mar 92 15:20:32 GMT
Article-I.D.: unx.1992Mar11.152032.2317
References: <i4fhbhs@sgi.sgi.com>
Sender: news@unx.sas.com (Noter of &worthy Events)
Organization: (none)
Lines: 45
Nntp-Posting-Host: shewhart.unx.sas.com
Originator: sasrdt@shewhart.unx.sas.com
X-Bytes: 2186

In article <i4fhbhs@sgi.sgi.com>, rpw3@rigden.wpd.sgi.com (Rob Warnock) writes:
|> sasrdt@shewhart.unx.sas.com (Randall D. Tobias) writes:
|> +---------------
|> | Where can I find a source which explains how Rivest's MD[45]
|> | cryptographic hashing algorithms work?
|> +---------------
|>
|> It doesn't matter.

Huh?  How can I judge the truth of their conjecture about its
irreversibility?  It looks to me like a rather arbitrary assortment of
bit-munging operations.  Are they really chosen "randomly" (in a sense)
or is there some reason for choosing these bit-munging operations and
not others?  The sqrt(2) and sqrt(3) which show up look particularly
suspect: why not pi or e or (10! mod 2^16)?  I know from statistics
that randomly-generated random number generating algorithms rarely work
all that well.  All I want to know is why it is thought to work well,
besides on the basis of empirical observation.

|> +---------------
|> | A certain up-date operation we might do takes a long time, and we don't
|> | want to have to do it if in fact the file hasn't changed, but we also
|> | don't want the overhead of a complete byte-by-byte comparison.
|> +---------------
|>
|> Cryptographic hashing functions examine *all* of the data and do complicated
|> things on it (usually several times). A byte-by-byte comparison will always
|> be *much* cheaper, assuming you have the pervious version locally.
|>
|> Now if the two versions to be compared are very far away from each other
|> and the cost of computing a complex hash function over the whole file is
|> less than the cost of transmitting the file... But that's another story.

I wasn't clear enough.  Part of the "overhead" which we don't want is
making the old versions of files available for comparison in the first
place.  Shipping hashed digests of old versions is to be a substitute
for shipping megabytes and megabytes of old versions themselves.
-- 

Randy Tobias               SAS Institute Inc.     (919) 677-8000 x7933
sasrdt@dev.sas.com         SAS Circle             (919) 677-8224 (Fax)
72450.2545@compuserve.com  Cary, NC   27512-8000

 ... just my $(-exp(2*sqrt(-1)*arcos(0))/(((2**(2 + 1)) - 1)**2 + 1)).
