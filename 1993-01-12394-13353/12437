Path: msuinfo!caen!spool.mu.edu!olivea!charnel!rat!polyslo.csc.calpoly.edu!rteasdal
From: rteasdal@polyslo.csc.calpoly.edu (Rusty)
Newsgroups: sci.crypt
Subject: Another well-intentioned novice's question
Message-ID: <1993Jan04.051300.26089@rat.csc.calpoly.edu>
Date: 4 Jan 93 05:13:00 GMT
Organization: Cal Poly San Luis Obispo, CSc Department
Lines: 36
Nntp-Posting-Host: polyslo.csc.calpoly.edu



	Hmmmm. I've been pondering this question for some time
now, and making little headway, so I thought I'd better run it
by the net's crypto wizards:

	A few months ago, in the letters section of the 
_Communications of the ACM_, there was published a letter which
commented that, in the opinion of the author, cryptographic	
efficiency would be greatly enhanced by compressing a message
before encrypting it, the better to remove repetitive patterns
from the plaintext.

	I don't see, though, that standard (i.e. LZW) compression
algorithms actually do remove any of the redundancy from the text;
they merely encode and substitute for lengthy repetitive patterns
with smaller ones. Mathematically, the whole of the message is
still there, is it not?

	Admittedly, if one does not know the full details of
the algorithm used in the compression, decompression will be far
from easy, and I can certainly conceive of several simple hacks to
the basic LZW technique which would be tantamount to simple shift
encoding and the like, making it harder yet to unravel.

	Can someone more learned elucidate at greater length on
this question? And is there any merit at all to the original letter 
writer's argument?



	 
-- 
||||||||   Russ Teasdale -- rteasdal@polyslo.CalPoly.EDU  --  (Rusty)  ||||||||
-------------------------------------------------------------------------------
"Gentlemen, if we do not succeed, then we run the risk of failure." - D. Quayle
