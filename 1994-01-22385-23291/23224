Newsgroups: sci.crypt
Path: msuinfo!agate!library.ucla.edu!europa.eng.gtefsd.com!howland.reston.ans.net!cs.utexas.edu!swrinde!elroy.jpl.nasa.gov!decwrl!netcomsv!netcom.com!phr
From: phr@netcom.com (Paul Rubin)
Subject: Re: Nuclear random number generator
Message-ID: <phrCKB149.JpF@netcom.com>
Organization: Netcom - Online Communication Services (408 241-9760 guest)
References: <sy13=gm@dixie.com> <phrCK7DKs.8BK@netcom.com> <yv335j#@dixie.com>
Date: Thu, 27 Jan 1994 20:04:56 GMT
Lines: 75

In article <yv335j#@dixie.com> jgd@dixie.com (John De Armond) writes:
>phr@netcom.com (Paul Rubin) writes:
>
>>This would be kind of a fun experiment, although of course random
>>numbers ftp'd over a big network would be useless for cryptographic
>>purposes (the idea of cryptography is that the random numbers have
>>to be kept secret from eavesdroppers).  

>I've been thinking about this.  Suppose the file has, say, a million
>random numbers.  Or a hundred million or some such huge number.
>Suppose I want to do a one time pad cypher.  Could I take two 
>random chunks of the random number string the length of the plaintext, 
>XOR them together and get another equally random but now secret pad? 

No.  The idea of a random OTP is that it is really totally random.
If you xor some subsets (you *really* don't want contiguous
chunks) of a static random file, you have to choose the subsets
somehow.  The data that identifies the choice of subsets could
be thought of as a conventional cryptographic key.

>How does this appear from a cryptanalysis perspective?

Solving the mapping between the XOR'd subsets and the original
subsets becomes a standard cryptanalysis problem.  It can be easy
or difficult depending on how your chunk-choosing algorithm
works.  You could choose an algorithm that makes the system
pretty secure, but if you do that, why bother with the big
random file instead of just using the algorithm as a conventional
encryption function?

If the chunks are contiguous, breaking the scheme is very easy.  Say
the attacker has a copy of the million-number file, plus an encryption
of some known plaintext.  First, he takes the file and sorts it
(treating it as 64-bit numbers, say) while remembering the original
position in the file for each number.  From the known plaintext he can
figure out chunk1^chunk2.  Call this sequence X.  Now with the sorted
file it is easy to find all the pairs from the original file that
xor'd together give X_1.  Chances are there is only one such pair; if
there's more than one, use X_2, X_3, etc. until a unique solution is
reached.  Since the chunks are contiguous, he has now discovered
the starting locations of both chunks and the system is broken.
(Whoops, this doesn't quite work as it requires both chunks to
start on 64-bit boundaries.  The simple fix is left as an exercise).

If you want to learn something about cryptography without going
out and buying a book, the sci.crypt FAQ that gets posted here
sometimes (look on sci.answers maybe) is a good place to start.

>
>>In practice I'd worry somewhat about sources of nonrandomness
>>introduced by the experimental apparatus somehow.  One thing you
>>could do is run the random bit stream from the previous step through
>>a cryptographic hash function like SHA.  This should get rid of
>>any correlations in it and hopefully not introduce any of its own.
>
>That's the problem I have.  If I take a series of counts over the same
>measurement interval, the values will distribute themselves around the 
>mean in a Poisson distribution. This is characteristic and is used
>as a quality check for radiation detectors.  Therefore the counts will 
>not be random.  The first alternative that comes to mine is to have
>each individual count trigger a read on a high speed counter, say a 
>tight loop in a program.  To my novice mind, this should produce
>a random stream of numbers.  True?  And how do I test the randomness?

Well the counts will be random, it's just that they will be random
from a nonuniform distribution.  Checking for the Poisson distribution
as a quality test sounds like a good idea!  Yes, counting the events
in some time interval and taking the low order few bits sounds like it
should result in a bit stream that is very random.  I'm more concerned
about errors introduced by the instrumentation electronics somehow
(a subject about which I know close to zero).  See the introduction
to the Rand Corporation's "A million random digits with 100,000
normal deviates" (a random number table produced in the 1950's
with an electronic roulette wheel) for discussion of some sources
of these kinds of errors.  Your local math library should have a copy.
